---
icon: lucide/shield
tags:
  - Government
  - Policy
  - Containment
  - Resilience
  - Infrastructure
---

# National Security Considerations


---

## Overview

Advanced AI and AGI present distinct challenges for national security, defence and intelligence agencies. This page outlines how the [C·A·G·R framework](../framework/index.md) applies in national security contexts, where the stakes, adversaries and risk tolerances differ from civilian regulation.

**Key insight:** National security agencies face AI risks in three ways:
1. **As potential victims:** AI-enabled threats targeting Australian interests
2. **As users:** Deploying AI systems in defence, intelligence and security functions
3. **As strategic actors:** Shaping how advanced AI affects international stability and Australian security

This page addresses all three.

### Quick reference: Key AI threats and defences

| **Threat** | **Impact** | **Primary C·A·G·R response** | **Australian priority** |
|------------|-----------|----------------------------|------------------------|
| **AI-enabled cyber operations** | Faster, more sophisticated, scaled attacks | Resilience (cyber defence), Containment (limit adversary capabilities) | High—critical infrastructure vulnerable |
| **Information operations** | Deepfakes, personalised disinformation, erosion of trust | Resilience (media literacy, trusted sources), Governance (content standards) | High—small media market, high social media use |
| **AI arms racing** | Speed vs. safety trade-offs, strategic instability | Governance (international norms), Alignment (human control) | Medium—not major developer but ally of US |
| **Autonomous weapons** | Loss of human control, accountability gaps | Alignment (meaningful human control), Governance (legal frameworks) | Medium—strong humanitarian law tradition |
| **Intelligence AI failures** | Over-reliance, bias, adversarial manipulation | Alignment (decision support not replacement), Resilience (human backup) | Medium—efficiency valuable but limited redundancy |

---

### The AI Pentathlon (Not just a "Race")

Thinking of AI competition as a single race is misleading. As strategists [Kahl and Mitre](https://www.foreignaffairs.com/united-states/china-real-artificial-intelligence-race-innovation) argue, it is better understood as a **pentathlon**—five separate contests with different goals:

1.  **Innovation:** Developing the most capable models.
2.  **National Security Integration:** Adopting AI effectively into defence/intel workflows.
3.  **Economic Adoption:** Diffusing AI across the broader economy.
4.  **Global AI Stack Leadership:** Controlling the hardware, cloud, and talent layers.
5.  **Race to the Bottom:** The danger that safety standards are cut to gain speed.

**Implication for Australia:** We may not win the "Innovation" or "Stack Leadership" events against superpowers. But we **must** excel in the "Integration," "Adoption," and avoiding the "Race to the Bottom" (by maintaining high safety standards) to secure our national interests.

---

## Why AI and AGI matter for national security

### The strategic environment is changing

AI is not just another technology. It may reshape:

- **Military capability and deterrence:** Autonomous systems, AI-enabled decision support, cyber operations
- **Intelligence collection and analysis:** Processing vast data, pattern detection, predictive analysis
- **Information integrity:** AI-generated disinformation, deepfakes, manipulation at scale
- **Economic and technological competition:** AI leadership increasingly tied to national power
- **Strategic stability:** Possibility of AI arms races, accidents, miscalculation

### Australia's position

**We are not a major AI developer.** The most capable systems come from the US and (increasingly) China. But we are:

- A close ally of the leading AI power (US)
- Dependent on AI-enabled systems for defence, intelligence and critical infrastructure
- Vulnerable to AI-enabled threats (cyber, information operations, economic coercion)
- A contributor to international efforts to manage AI risks

**Key question for Australian security:** How do we navigate a world where we depend on but don't control the most advanced AI systems?

---

## National security applications of C·A·G·R

The four pillars apply in security contexts but with different emphasis and methods.

### :lucide-shield-ban: Containment (in security contexts)

**Goal:** Prevent adversaries from acquiring dangerous AI capabilities; ensure friendly AI systems don't escape control.

**Defence and intelligence role:**

**1. Threat intelligence on AI capabilities**
- Track adversary AI development (capabilities, timelines, intent)
- Assess which AI capabilities pose strategic threats
- Monitor supply chains for advanced AI chips and systems
- Provide intelligence to support export controls and containment measures

**2. Support compute governance and export controls**
- Intelligence on circumvention attempts
- Assessment of compliance by state and non-state actors
- Understanding of black market supply chains
- Verification of international agreements

**3. Protecting model weights and sensitive AI systems**
- Treat advanced model weights as sensitive national security assets
- Counter foreign intelligence collection targeting AI capabilities
- Insider threat programs for organisations with access to frontier systems
- Secure facilities for development and testing of national security AI systems

**4. AI control for security applications**
- Ensuring military and intelligence AI systems can be monitored and restricted
- Testing whether AI control methods work under adversarial conditions
- Maintaining human oversight of high-consequence decisions (targeting, intelligence assessments)
- Developing shutdown and rollback procedures that work in operational environments

### :lucide-target: Alignment (in security contexts)

**Goal:** Ensure AI systems used in national security functions behave safely and as intended, even under stress and adversarial conditions.

**Defence and intelligence role:**

**1. Red-teaming and adversarial testing**
- Test AI systems against sophisticated adversarial attacks
- Simulate how adversaries might exploit or manipulate AI systems
- Evaluate robustness of alignment under operational stress (time pressure, incomplete information, active deception)
- Identify failure modes before deployment in high-stakes environments

**2. Meaningful human control**
- Maintain human decision-making authority over high-consequence actions
- Ensure AI provides decision support, not autonomous decision-making, where stakes are high
- Design systems that enable operators to understand and override AI recommendations
- Balance speed (AI advantage) with control (human judgment on context and ethics)

**3. Testing alignment under realistic conditions**
- Security environments differ from civilian: adversaries, deception, fog of war
- Systems must be tested under realistic operational conditions, not just lab settings
- Need to verify alignment doesn't degrade under distributional shift (situations different from training)

### :lucide-scale: Governance (in security contexts)

**Goal:** Clear rules, oversight and accountability for AI in national security functions.

**Defence and intelligence role:**

**1. Internal governance of AI use**
- Clear policies on when and how AI can be used in operations
- Approval processes for deploying new AI capabilities
- Audit and oversight of AI system performance
- Accountability when AI-enabled operations cause harm

**2. International norms and agreements**
- Participate in discussions on AI arms control and stability measures
- Advocate for norms around dangerous AI applications (e.g., autonomous weapons, AI-enabled bioweapons)
- Support verification mechanisms for international commitments
- Balance: security interests vs. proliferation risks vs. treaty obligations

**3. Civil-military coordination**
- Security agencies often develop or access capabilities earlier than civilian sector
- Need mechanisms to share threat intelligence with civilian regulators without compromising sources
- Ensure military/intelligence AI governance connects to whole-of-government approach
- Balance: security classification needs vs. democratic oversight

**4. Oversight and accountability**
- Parliamentary oversight of AI use in national security functions
- Inspector-General mechanisms for intelligence agencies using AI
- Public transparency where possible; classified oversight where necessary
- Ensuring democratic accountability even for classified capabilities

### :lucide-shield: Resilience (in security contexts)

**Goal:** Maintain essential security functions when AI systems fail or are attacked; withstand AI-enabled threats.

**Defence and intelligence role:**

**1. Resilience against AI-enabled threats**
- Cyber defence against AI-assisted attacks (faster, more sophisticated, scaled)
- Information operations defence (detecting and countering AI-generated disinformation)
- Physical security against AI-enabled threats (drones, autonomous systems)
- Intelligence on adversary AI capabilities to support defensive planning

**2. Resilience of security systems that use AI**
- Maintain manual capabilities when AI systems fail or are compromised
- Regular exercises testing fallback procedures
- Avoiding over-dependence on AI where failure would be catastrophic
- Ensuring human operators retain skills to operate without AI assistance

**3. Strategic resilience and continuity**
- Ensuring continuity of national security functions if AI systems fail
- Protecting critical decision-making from AI manipulation or failure
- Maintaining alternative communication and coordination methods
- Planning for scenarios where AI systems fail simultaneously across multiple functions

**4. Community resilience to security threats**
- Public understanding of AI-enabled threats (disinformation, deepfakes)
- Building societal resilience against information manipulation
- Clear communication during crises when AI systems fail or are attacked
- Supporting community preparedness for disruptions

---

## Australian Context for National Security AI

**Containment:**
- Limited unilateral leverage—most effective through alliances (Five Eyes intelligence sharing)
- ASD and ASIO provide intelligence collection and counterintelligence capabilities
- Defence industry and critical infrastructure are potential targets for foreign acquisition

**Alignment:**
- Defence has capability for realistic adversarial testing
- Strong norms around human control of lethal force worth preserving
- Often adopt partner systems (US, UK)—independent evaluation still essential

**Governance:**
- Strong traditions of parliamentary and inspector-general oversight
- Close allied integration requires coordination on governance standards
- As adapters not developers, governance focuses on acquisition and deployment
- Australian law and values must apply even when using partner systems

**Resilience:**
- Geographic isolation provides some resilience but creates vulnerabilities (smaller population, longer supply chains)
- Strong civil-military coordination tradition enables whole-of-nation resilience
- Opportunity to position resilience as both national security and community issue

---

## Defence-in-depth for national security

!!! tip "Key insight: Layers of protection"
    Australia cannot prevent all dangerous AI development (Layer 1). We can influence but not fully control deployment standards (Layer 2). But we can and must build resilience to withstand AI-enabled threats and failures (Layer 3). This is where Australian sovereignty is strongest.

The three-layer model applies with security-specific interpretations:

**Layer 1: Prevent dangerous AI capabilities from being developed**
- Intelligence collection on adversary AI development
- Support for international norms and agreements limiting dangerous applications
- Export controls and compute governance to prevent proliferation
- Engagement in AI Safety Institutes and technical cooperation with allies

**Layer 2: Constrain dangerous capabilities and deployments**
- Evaluation and red-teaming of security AI systems before deployment
- AI control methods for high-consequence applications
- Meaningful human control requirements for lethal force and critical decisions
- Secure facilities and insider threat programs for sensitive AI development

**Layer 3: Withstand AI-enabled threats and AI failures**
- Cyber defences adapted for AI-assisted attacks
- Information operations defence and community resilience
- Continuity plans for when AI systems fail or are compromised
- Manual capabilities maintained as fallback
- Strategic resilience and crisis communication

**For security planning:** Layer 3 is essential even if Layers 1 and 2 fail. Australia must be able to maintain security functions regardless of what happens with frontier AI development overseas.

---

## Key national security challenges

=== "Cyber operations"

    **The threat:** Advanced AI enables cyber operations that are faster/scaled (automated reconnaissance at scale), more sophisticated (AI-assisted vulnerability discovery), and harder to attribute (AI-generated obfuscation).

    **C·A·G·R application:**

    - **Containment:** Prevent adversaries acquiring capabilities beyond current defences
    - **Alignment:** Ensure defensive AI behaves reliably under adversarial conditions
    - **Governance:** Rules of engagement; international norms
    - **Resilience:** Cyber defences for AI-assisted attacks; continuity when compromised

    **Australian considerations:** ASD has strong capability but must adapt to AI-enabled threats. Critical infrastructure vulnerable; attribution harder.

=== "Information operations"

    **The threat:** AI enables information operations at unprecedented scale—deepfakes and personalised disinformation, automated influence operations, erosion of trust in all information.

    **C·A·G·R application:**

    - **Containment:** Limit adversary access; watermarking and provenance systems
    - **Alignment:** Ensure platforms don't amplify disinformation
    - **Governance:** Content standards; transparency; electoral integrity measures
    - **Resilience:** Media literacy; trusted sources; community resilience

    **Australian considerations:** Small media market + high social media use = increased vulnerability. Strong institutions provide foundation.

=== "Arms racing & stability"

    **The concern:** Competition for military AI superiority creates dangerous dynamics—speed vs. safety trade-offs, accidents/miscalculation, erosion of human control.

    **C·A·G·R application:**

    - **Containment:** Support international efforts to avoid arms racing; verification
    - **Alignment:** Maintain meaningful human control; robustness under stress
    - **Governance:** International norms; transparency and confidence-building
    - **Resilience:** Maintain stability as capabilities advance; crisis communication

    **Australian considerations:** Not major developer but close US ally. Can advocate for stability without competitive perception. Support arms control.

=== "Autonomous weapons"

    **The challenge:** Can humans maintain control over targeting decisions? Can AI reliably distinguish combatants from civilians? Who's responsible when autonomous systems cause unlawful harm?

    **C·A·G·R application:**

    - **Containment:** Limits on development/proliferation
    - **Alignment:** Ensure compliance with international humanitarian law
    - **Governance:** Legal frameworks; oversight; international agreements
    - **Resilience:** Defences against adversary autonomous systems

    **Australian considerations:** Strong humanitarian law support; clear Defence doctrine requiring human control of lethal force. Opportunity to lead on international norms.

=== "Intelligence AI"

    **The opportunity and risk:** AI enables processing vast data, predictive analysis, faster response—but creates risks of over-reliance/bias, adversarial manipulation, uncontrolled escalation.

    **C·A·G·R application:**

    - **Containment:** Control over AI for sensitive intelligence functions
    - **Alignment:** AI provides decision support, not replacement for human judgment
    - **Governance:** Clear rules on AI intelligence use; accountability
    - **Resilience:** Maintain human analytical capability as backup

    **Australian considerations:** Smaller intelligence community means efficiency gains valuable but less redundancy if AI fails. Five Eyes sharing: need to evaluate AI-generated intelligence from partners.

---

## Integration with civilian AI governance

AI risks don't neatly separate into "national security" and "civilian." Many issues span both:

**Shared risks:**

- Critical infrastructure uses AI and is a security target
- Information platforms are both commercial and potential vectors for foreign influence
- Compute governance affects both commercial and military AI
- Evaluation methodologies apply to civilian and security systems

**Coordination mechanisms:**

**1. Information sharing:**

- Security agencies share threat intelligence with civilian regulators (at appropriate classification)
- Civilian regulators share information about AI incidents and vulnerabilities with security agencies
- Protected channels for sensitive information that affects both domains

**2. Joint risk assessment:**

- National risk assessments bringing together security and civilian perspectives
- Shared horizon scanning for emerging AI risks
- Coordinated evaluation of AI systems with both security and civilian applications

**3. Coordinated governance:**

- Security agencies participate in whole-of-government AI coordination mechanisms
- Civilian regulators consulted on security AI governance where appropriate
- Aligned approaches to international engagement and standards-setting

**4. Shared capability development:**

- Evaluation facilities accessible to security and civilian regulators (with appropriate security)
- University partnerships that build capability relevant to both domains
- Training and skill development shared across government

**Australian considerations:**

- Strong institutions and inter-agency trust enable effective coordination
- Existing coordination mechanisms (National Security Committee, emergency management structures) provide models
- Need balance: security agencies need operational security; civilian regulators need transparency
- Opportunity to position AI governance as integrated national challenge, not siloed issue

---

## What makes national security AI governance effective

Security AI governance works when it integrates with rather than duplicates civilian approaches, while preserving operational security and decision advantage.

**Build evaluation capability early.** Security agencies need the ability to independently assess AI systems—both those they use and those adversaries might deploy. This requires technical expertise, realistic testing environments, and clear evaluation frameworks developed before crisis conditions force rushed deployments.

**Maintain human judgment where it matters.** AI offers speed and scale advantages, but security decisions often require contextual understanding, ethical judgment, and accountability that AI cannot provide. The pressure to match adversary speed is real, but removing humans from high-consequence decisions creates risks that outweigh tactical advantages.

**Integrate with whole-of-government coordination.** Security agencies have unique intelligence and capabilities, but AI risks don't respect the security-civilian boundary. Effective governance requires mechanisms to share threat intelligence with civilian regulators, participate in coordinated evaluations, and align approaches to international engagement—all while managing classification sensitivities.

**Prepare for AI failures and attacks.** Resilience is where Australian sovereignty is strongest. Security functions must remain viable when AI systems fail or are compromised. This means maintaining manual capabilities, testing fallback procedures, and ensuring operators retain skills to operate without AI assistance—investments that pay off regardless of how frontier AI develops overseas.

**Balance classification with accountability.** Democratic oversight of security AI use is essential, even for classified capabilities. Inspector-General mechanisms, parliamentary oversight with appropriate security clearances, and public transparency where possible ensure democratic accountability without compromising operational security.

For detailed analysis of security AI governance considerations, see the [For Researchers](../for-researchers.md) section.

---

## Sources & Further Reading

??? note "AI and strategic stability"

    **International security implications:**
    - Brundage et al. (2018) ["The Malicious Use of Artificial Intelligence"](https://maliciousaireport.com/)
    - Horowitz, Allen & Kania (2018) ["Strategic Competition in an Era of Artificial Intelligence"](https://www.cnas.org/publications/reports/strategic-competition-in-an-era-of-artificial-intelligence)
    - Scharre (2023) ["Four Battlegrounds: Power in the Age of Artificial Intelligence"](https://www.cnas.org/publications/reports/four-battlegrounds)
    - Zwetsloot & Dafoe (2019) ["Thinking About Risks From AI: Accidents, Misuse and Structure"](https://www.governance.ai/research-paper/thinking-about-risks-from-ai)

    **Strategic stability and arms control:**
    - [UNIDIR resources on AI and security](https://www.unidir.org/programmes/artificial-intelligence-and-security)
    - Boulanin & Verbruggen (2017) ["Mapping the Development of Autonomy in Weapon Systems"](https://www.sipri.org/publications/2017/other-publications/mapping-development-autonomy-weapon-systems) (SIPRI)
    - Geist & Lohn (2018) ["How Might Artificial Intelligence Affect the Risk of Nuclear War?"](https://www.rand.org/pubs/perspectives/PE296.html) (RAND)

??? note "Autonomous weapons and meaningful human control"

    **International humanitarian law:**
    - [Campaign to Stop Killer Robots](https://www.stopkillerrobots.org/) resources
    - [ICRC position on autonomous weapons](https://www.icrc.org/en/document/autonomy-artificial-intelligence-and-robotics-technical-aspects-human-control)
    - Moyes (2016) ["Key Elements of Meaningful Human Control"](https://article36.org/wp-content/uploads/2016/04/MHC-2016-FINAL.pdf) (Article 36)
    - Boulanin, Bruun & Goussac (2021) ["Autonomous Weapon Systems and International Humanitarian Law"](https://www.sipri.org/publications/2021/other-publications/autonomous-weapon-systems-and-international-humanitarian-law) (SIPRI)

    **Australian policy:**
    - [Australian Defence Force policy on autonomous weapons](https://www.defence.gov.au/)
    - Crootof (2016) ["A Meaningful Floor for Meaningful Human Control"](https://digitalcommons.law.yale.edu/fss_papers/5362/) (Temple International & Comparative Law Journal)

??? note "AI-enabled cyber and information operations"

    **Cyber security:**
    - [Australian Cyber Security Centre (ACSC) threat reports](https://www.cyber.gov.au/)
    - Brundage et al. (2018) ["The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation"](https://maliciousaireport.com/) (Chapter on cyber security)
    - Schneier (2023) ["AI Will Increase the Quantity—and Quality—of Phishing Scams"](https://www.schneier.com/blog/archives/2023/04/ai-will-increase-the-quantity-and-quality-of-phishing-scams.html)

    **Information operations:**
    - Chesney & Citron (2019) ["Deep Fakes: A Looming Challenge for Privacy, Democracy, and National Security"](https://doi.org/10.17863/CAM.38952)
    - Goldstein et al. (2023) ["Generative Language Models and Automated Influence Operations"](https://arxiv.org/abs/2301.04246)
    - [Australian Strategic Policy Institute (ASPI) work on disinformation](https://www.aspi.org.au/topics/disinformation)
    - Stocking & Sumida (2018) ["Social Media Bots Draw Public's Attention and Concern"](https://www.pewresearch.org/short-reads/2018/10/15/social-media-bots-draw-publics-attention-and-concern/) (Pew Research)

??? note "Intelligence and AI"

    **AI in intelligence analysis:**
    - [ODNI resources on AI](https://www.dni.gov/) (US Office of the Director of National Intelligence)
    - Johnson (2019) ["Artificial Intelligence & Future Warfare"](https://www.esd.whs.mil/Portals/54/Documents/FOID/Reading%20Room/NCB/19-F-0505_Artificial_Intelligence_Future_Warfare.pdf)
    - Walsh (2019) ["Algorithms and Insight: The Limits of Big Data and Artificial Intelligence for Intelligence Analysis"](https://ieeexplore.ieee.org/document/8890890)

    **Five Eyes cooperation:**
    - [Five Eyes intelligence sharing arrangements](https://www.intelligence.gov.au/about-us/our-partnerships/five-eyes)
    - Understanding of joint approaches to AI in intelligence and security contexts

??? note "Australian national security context"

    **Australian national security strategy:**
    - [National Security Strategy](https://www.homeaffairs.gov.au/about-us/our-portfolios/national-security/security-coordination/national-security-strategy)
    - [Defence Strategic Review](https://www.defence.gov.au/about/reviews-inquiries/defence-strategic-review)
    - [Australian Signals Directorate (ASD)](https://www.asd.gov.au/) threat assessments
    - [Australian Security Intelligence Organisation (ASIO) Annual Report](https://www.asio.gov.au/publications/annual-report)

    **Critical infrastructure:**
    - [Security of Critical Infrastructure Act 2018](https://www.legislation.gov.au/Details/C2022C00310)
    - [Critical Infrastructure Centre](https://www.homeaffairs.gov.au/about-us/our-portfolios/national-security/security-coordination/critical-infrastructure-centre)

    **Regional security:**
    - [ASPI Indo-Pacific strategy papers](https://www.aspi.org.au/)
    - [Department of Foreign Affairs and Trade regional security resources](https://www.dfat.gov.au/)

---

