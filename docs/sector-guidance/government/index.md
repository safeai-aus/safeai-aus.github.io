---
icon: lucide/landmark
tags:
  - Government
  - Policy
  - Guidance
---

# Government & Policy

## Who this is for

Australian government and public institutions: federal/state/local governments, regulators, public sector agencies, universities, national security and defence.

---

## The challenge

Advanced AI systems are being embedded in critical services, infrastructure and decision-making across Australia. We import most frontier AI but experience its full impact—from algorithmic decisions in public services to AI-enabled threats that could overwhelm our institutions.

**Key realities:**

- AI is already deployed in frontline services (welfare, policing), critical infrastructure (energy, logistics), and information ecosystems
- As capabilities scale, systems could automate faster than institutions adapt, be weaponised at scale, or become difficult to meaningfully oversee
- Australia is a net importer of frontier AI—we don't control development but face full impact of deployment

---

## Dealing with uncertainty: Options, not odds

Debates about AI timelines often lead to policy gridlock. Instead of trying to predict exactly *when* advanced capabilities will emerge (the "odds"), policymakers should focus on **expanding the portfolio of actions** (the "options") available to Australia.

This approach, drawing on [strategic work by the RAND Corporation](https://geopoliticsagi.substack.com/p/taking-agi-seriously-not-literally), manages uncertainty by categorising actions into three types:

### 1. No-regret options
**"Good to do in any future."**
Investments that make Australia safer and more prosperous whether AGI arrives in 3 years or 30.
*   **Examples:** Strengthening cyber-resilience; diversifying critical supply chains; investing in technical safety talent; creating clear liability frameworks; auditing existing algorithmic decision-making.

### 2. Break-glass plans
**"Prepared for the worst."**
Capabilities developed now but kept in reserve for emergencies or rapid shifts in the threat landscape. These might be politically or economically costly to use today, but essential if safety assumptions fail.
*   **Examples:** Mechanisms to rapidly restrict compute access; protocols for emergency isolation of critical digital infrastructure; emergency "kill switches" for interconnected systems; severe capital controls on AI sectors detailed in the [National Security](national-security.md) section.

### 3. Signposts
**"Knowing when to switch."**
Clear indicators that signal when to move from "business as usual" to "break-glass" modes.
*   **Examples:** Evidence of uncontrolled self-replication; models breaking state-of-the-art encryption; loss of control incidents in allied nations; adversaries deploying fully autonomous lethal systems.

**Strategic value for Australia:** As a net importer of AI, Australia cannot purely "regulate" its way to safety. We must build **adaptive capacity**—the ability to recognize signposts and switch to break-glass plans faster than threats can escalate.

---

## How sectors work together

Government doesn't manage AI risks alone. Effective AI governance requires coordination across all three sectors:

**Government creates the framework** through regulation, standards and procurement requirements. But [business](../business-industry/index.md) implements these in practice, and [communities](../communities-households/index.md) experience the impacts and provide democratic input on what's acceptable.

**Business decisions cascade to communities:** Corporate AI deployment affects local employment, service access and information ecosystems. [Business resilience planning](../business-industry/agi-preparedness-strategy.md) must account for community dependencies.

**Community resilience supports national resilience:** Strong [local networks](../communities-households/community-preparedness.md) reduce the burden on government services during disruptions and provide early warning when systems aren't working as intended.

**Key insight:** Your policy choices shape business strategy and community outcomes. Understanding [what business needs](../business-industry/index.md) and [what communities can do](../communities-households/index.md) makes for better governance.

---

## Government's role through C·A·G·R

### :lucide-shield-ban: Containment

Preventing dangerous systems from being deployed without proper evaluation.

**What this involves:**

- Licensing requirements for high-risk systems
- Independent safety evaluations before deployment
- Contributing to international compute governance
- Using procurement standards to demand strong safety properties

[Learn more →](../framework/containment.md)

### :lucide-target: Alignment

Ensuring systems behave safely in Australian contexts.

**What this involves:**

- Evaluation capability to test systems under realistic and adversarial conditions
- Independent assessment of alignment properties
- Funding for alignment research relevant to local priorities
- Incident reporting that captures why approaches failed

[Learn more →](../framework/alignment.md)

### :lucide-scale: Governance

Designing laws, institutions and coordination mechanisms.

**What this involves:**

- Risk-based regulatory frameworks
- Clear institutional roles and coordination mechanisms
- Transparency, incident reporting, and audit capabilities
- Participation in international efforts to avoid capability races
- Adequately resourced regulators with clear authority

[Learn more →](../framework/governance.md)

### :lucide-shield: Resilience

Building capacity to withstand and recover from AI-related disruptions.

**What this involves:**

- Integrating AI risks into national risk registers and emergency plans
- Stress-testing critical services using disruption scenarios
- Maintaining manual fallbacks and testing them regularly
- Supporting community preparedness
- Coordinating whole-of-government response capability

[Learn more →](../framework/resilience.md)

---

## Defence in depth: Three layers

**Layer 1: Prevent dangerous AI training**
Stop dangerous systems from being built through international cooperation and compute governance.
*Australia's role:* Influence through partnerships, supply chains, procurement standards.

**Layer 2: Constrain dangerous capabilities**
Ensure systems are safe and controlled through licensing, evaluations, and regulation.
*Australia's role:* Strong sovereignty. Our laws govern deployment here.

**Layer 3: Withstand dangerous actions**
Maintain essential functions through continuity planning and manual fallbacks.
*Australia's role:* Core sovereignty. Must build this capability regardless of what happens elsewhere.

[See full framework →](../framework/index.md)

---

## Current Australian context

**Australian AI Safety Institute (AISI)**

In November 2025, the Australian Government announced the establishment of the Australian AI Safety Institute with $29.9 million in funding. The AISI will be operational in early 2026 and will:

- Test and evaluate advanced AI systems for safety
- Develop guidance and standards for AI development and deployment
- Participate in the International Network of AI Safety Institutes
- Work with researchers, industry and government on AI safety

**National AI Plan (December 2025)**

The National AI Plan outlines Australia's approach across three pillars:

- **Capture opportunity** — economic growth and productivity
- **Spread benefits** — ensuring AI benefits all Australians
- **Keep Australians safe** — managing risks and building trust

The AISI is a key part of the "Keep Australians safe" pillar, alongside voluntary safety standards and international engagement.

**About the guidance below:**

The following pages draw on international practice and AI safety research to outline **how different jurisdictions approach AI safety policy**. Rather than prescribing what Australia should do, they provide:

- **International comparisons** — how UK, US, EU, Singapore and others structure their approaches
- **Design considerations** — questions and trade-offs governments face when implementing AI safety frameworks
- **Options and examples** — different ways to address similar challenges, with their respective strengths and limitations

This is intended as a resource for **informing policy decisions** — comparing approaches, understanding trade-offs, and learning from international experience. Australia's choices will depend on our specific context, institutional strengths, resource constraints, and democratic priorities.

---

## Quick links to guidance

<div class="grid cards" markdown>

-   :lucide-file-text:{ .lg .middle } **[Policy Approaches](ai-safety-policy.md)**

    ---

    International comparisons: licensing, evaluations, transparency, compute governance, coordination and public funding

-   :lucide-building:{ .lg .middle } **[Institutions & Capability](institutions-and-capability.md)**

    ---

    Australian AI Safety Institute design, capability building, and institutional coordination

-   :lucide-network:{ .lg .middle } **[Coordination](coordination.md)**

    ---

    Whole-of-government coordination: agencies, jurisdictions, and international partnerships

-   :lucide-handshake:{ .lg .middle } **[Supporting Alternatives](supporting-alternatives.md)**

    ---

    Procurement, grants, and regulatory space for decentralised and cooperative AI

-   :lucide-shield:{ .lg .middle } **[National Security](national-security.md)**

    ---

    Strategic stability, AI-enabled operations, decision support, and break-glass options

</div>

