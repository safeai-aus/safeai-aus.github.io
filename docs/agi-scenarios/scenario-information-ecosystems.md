---
icon: lucide/globe
tags:
  - Scenario
  - Governance
  - Alignment
  - Democracy
---

# Scenario 3: AI, Democracy & Information Ecosystem Risks

![AI, Democracy & Information Ecosystem Risks](scenario-information-ecosystems-hero.jpg)

## Summary

By 2027, generative AI models can produce flawless video deepfakes, synthetic audio and persuasive written content at near-zero marginal cost. Political campaigns, advocacy groups and foreign actors use these tools to micro-target Australian voters on social media.

During the 2028 federal election campaign, a sophisticated deepfake video surfaces showing a major party leader making inflammatory remarks about Australian values. The video circulates on Facebook, WhatsApp and X. Within hours it has 3 million views. Australia's small fact-checking organisations scramble to verify it, but their debunks reach only a fraction of the audience—social media algorithms favour engagement over accuracy, and users in partisan bubbles rarely see corrections.

Then the counter-narratives begin: competing deepfakes, fake fact-checks, synthetic "eyewitness" accounts. By election day, voters in marginal electorates like Bass, Eden-Monaro and Gilmore have seen dozens of contradictory videos and can't tell what's real.

**The pattern repeats.** By 2031, sophisticated synthetic content floods every election cycle. Trust in ABC, Nine, News Corp and fact-checkers erodes as they're unable to verify content fast enough. Australians increasingly retreat to partisan information bubbles where AI-generated content reinforces existing beliefs.

When verification becomes impossible for most citizens and expensive for institutions, democracy itself is under strain.

!!! warning "This is a scenario, not a prediction"

    This scenario explores how AI-generated content could degrade trust in information and democratic processes. The specific timeline and incidents are illustrative—but the underlying capability (cheap, convincing synthetic media) already exists and is improving rapidly.

!!! info "Threat pathways"
    This scenario combines three pathways that degrade democratic processes:

    **Catastrophic misuse** – AI-powered information operations flood elections with synthetic content at scale

    **Power concentration** – Platform governance choices shape information environment more than democratic oversight

    **Gradual disempowerment** – Citizens lose ability to distinguish real from synthetic; trust in institutions erodes

---

## Implications for C·A·G·R Framework

This scenario shows how AI-generated content degrades trust in information and democratic processes. The challenge is maintaining shared understanding of reality when verification becomes difficult and manipulation becomes easy.

=== ":lucide-shield-ban: Containment"

    - Difficult to prevent the development of persuasive content generation capabilities
    - Once models exist, jailbreaking and fine-tuning can bypass safety measures
    - Open-source models make content generation widely accessible
    - No effective way to contain synthetic media creation at scale

=== ":lucide-target: Alignment"

    - Models are optimised for engagement or persuasion, not for democratic health
    - Safety mitigations may reduce obvious harms but miss aggregate, long-term effects on trust
    - Even "aligned" systems can be fine-tuned or prompted for manipulative purposes

=== ":lucide-scale: Governance"

    - Existing electoral laws and media regulations are stretched by new forms of influence and campaigning
    - Platform governance choices shape the information environment more than democratic processes
    - Attribution and enforcement become nearly impossible at scale
    - Regulatory frameworks struggle to keep pace with evolving manipulation techniques

=== ":lucide-shield: Resilience"

    - Democracies depend on shared facts and norms of contestation
    - Persistent manipulation and confusion weaken these foundations
    - Communities and individuals need skills, tools and institutions to navigate an AI-saturated information environment
    - Media literacy and verification capabilities become critical societal infrastructure

---

## Questions for actors

Use these questions for risk assessments, strategic planning, and tabletop exercises.

=== ":material-bank: Government & Public Institutions"

    **Near-term (within 12 months):**

    - What would you do if a convincing deepfake of a minister making inflammatory statements circulated 36 hours before an election?
    - Who has authority to act? What legal powers exist? How fast could you respond?
    - How should electoral laws adapt when anyone can generate thousands of targeted messages at near-zero cost?
    - What verification mechanisms can you put in place for official government communications?
    - How can you ensure citizens can distinguish real government communications from fakes?

    **Strategic:**

    - What standards should apply to the use of AI in political communication by domestic actors?
    - How can public broadcasters, education systems and civic institutions support information resilience?
    - What role should government play in verifying information during crises or elections?

=== ":material-briefcase: Business & Industry"

    **Near-term (within 12 months):**

    - For all organisations: what's your crisis plan for when a deepfake CEO video circulates?
    - For platforms: what can you detect now? What will become undetectable in 6-12 months?
    - For platforms and media: what governance and transparency practices are appropriate for AI-generated political content?
    - For media organisations: how do you verify content when sophisticated fakes are indistinguishable from originals?
    - For all organisations: how do you protect brand reputation and customer trust in an environment of pervasive synthetic media?

    **Strategic:**

    - What verification infrastructure needs to exist across the media and technology ecosystem?
    - How can platforms balance free expression with preventing manipulation at scale?
    - What business models support quality journalism when AI can generate convincing but false content at near-zero cost?

=== ":material-account-group: Communities & Households"

    **Near-term (within 12 months):**

    - What local trusted sources and networks can you maintain that aren't mediated by social platforms?
    - How do you currently verify information before sharing it?
    - What simple habits can help: verify before sharing? Check multiple sources? Slow down on emotionally charged content?
    - How can community groups create spaces for deliberation that resist manipulation?

    **Strategic:**

    - How can communities strengthen media literacy without creating cynicism or disengagement?
    - What role can local trusted institutions (libraries, schools, community centres) play?
    - How do we maintain democratic participation when the information environment is compromised?

---

!!! question "Can't we just use AI to detect AI-generated content?"

    **Detection is an arms race we're likely to lose:**

    - As generation improves, detection becomes harder
    - Adversaries can test content against detectors before release
    - Even 95% detection accuracy leaves millions of undetected fakes
    - Attribution (who created it) is often impossible

    **More promising approaches:**

    - Cryptographic authentication for official sources (what's real, not what's fake)
    - Media literacy and slower information consumption habits
    - Trusted local networks that aren't mediated by platforms
    - Institutional resilience so democracy functions even with degraded information

    **Key insight:** We can't out-detect the problem—we need resilience to information degradation.

---

## Why this scenario matters for Resilience and Governance

This scenario shows **epistemic security** as critical infrastructure—democracy depends on shared reality and the ability to distinguish truth from falsehood.

**The challenge:**

- Containment of content generation capabilities is nearly impossible
- Alignment of models won't prevent misuse by bad actors
- Governance struggles when manipulation is cheap, attribution hard, and scale overwhelming
- Resilience becomes the primary defence: can democratic processes withstand information degradation?

**Key insights:**

1. **Verification is infrastructure:** Societies need ways to establish shared facts
2. **Trust is load-bearing:** When institutional trust erodes, verification becomes impossible
3. **Local networks matter:** Trusted relationships that aren't mediated by platforms become critical
4. **Media literacy is necessary but insufficient:** Can't expect individuals to verify everything

**Use this scenario to:**

- Test your information resilience: can your organisation function when public information is unreliable?
- Identify which trusted sources and relationships to maintain
- Design communications that remain verifiable under adversarial conditions
- Plan for maintaining democratic participation when information is degraded

---

??? note "Sources & Further Reading"
    This scenario draws from research on deepfakes, election integrity, synthetic media detection, and the challenges facing information ecosystems in democratic societies.

    **Australian precedents:** [Australian Electoral Commission](https://www.aec.gov.au/) disinformation monitoring · [RMIT FactLab](https://www.rmit.edu.au/about/schools-colleges/media-and-communication/industry/factlab) and [RMIT CrossCheck](https://www.rmit.edu.au/news/factlab/crosscheck) election fact-checking · [eSafety Commissioner](https://www.esafety.gov.au/) synthetic media regulatory considerations · [ABC Fact Check](https://www.abc.net.au/news/factcheck/)

    **Academic research:** Chesney & Citron (2019) ["Deep fakes: A looming challenge for privacy, democracy, and national security"](https://doi.org/10.17863/CAM.38952) · Wardle & Derakhshan (2017) ["Information disorder"](https://rm.coe.int/information-disorder-toward-an-interdisciplinary-framework-for-researc/168076277c) · Woolley & Howard (2018) *Computational Propaganda*

    **Policy organisations:** [Reset Australia](https://au.reset.tech/) · [First Draft News](https://firstdraftnews.org/) · [Centre for Responsible Technology](https://www.responsible.tech/) · [International Fact-Checking Network](https://www.poynter.org/ifcn/)

    **Case studies:** 2024 US and EU elections deepfake incidents · [Taiwan's approach to disinformation resilience](https://carnegieendowment.org/2024/02/08/taiwanese-model-for-generative-ai-governance-pub-91793) · Slovakia election deepfake (2023) · Indonesian election synthetic media (2024)

    **Key concepts:** See our [Concepts & Glossary](../concepts.md) for definitions of deepfakes, synthetic media, information operations, epistemic security and computational propaganda

---

