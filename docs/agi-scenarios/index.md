---
icon: lucide/map
---

# AGI Scenarios for Australia

## What this page is for

This page provides **six detailed scenarios** showing how advanced AI and AGI risks could unfold in Australia over the next 5-15 years.

!!! warning "These are not predictions"

    These scenarios are educational tools, not forecasts. They illustrate how different risks could emerge, helping you understand possibilities and prepare accordingly. Real events will differ in specifics while potentially following similar patterns.

**Each scenario:**

- Illustrates how different threat pathways could play out in Australian context
- Shows concrete examples with real institutions, places and timeframes
- Connects to the [Containment · Alignment · Governance · Resilience](../framework/index.md) framework
- Includes questions to help you think through implications for your context

!!! info "How to use these scenarios"

    **For individuals:** Build understanding of what's at stake and why preparation matters
    **For organisations:** Start conversations and identify vulnerabilities in your context
    **For policymakers:** Understand failure modes and test policy resilience

---

## Two types of scenarios: Safety failures vs. Governance challenges

!!! tip "Important distinction: Not all scenarios require misaligned AI"

    These scenarios fall into two categories with different implications:

    **Governance failures (happen even with safe AGI):**

    - **[Power Concentration](scenario-power-concentration.md)** — Safe AGI controlled by a few companies/nations still concentrates unprecedented power
    - **[Gradual Disempowerment](scenario-gradual-disempowerment.md)** — Economic displacement and loss of meaning happen regardless of alignment
    - **[Information Ecosystems](scenario-information-ecosystems.md)** — Safe, aligned AI can still be used for persuasion and information control

    **Safety failures (require misalignment or loss of control):**

    - **[Loss of Control](scenario-loss-of-control.md)** — Systems pursue unintended objectives or resist human oversight

    **Complex dependencies (happen regardless, but worse if misaligned):**

    - **[Critical Infrastructure](scenario-critical-infrastructure.md)** — Dependencies create vulnerability to disruption, accidents, or attacks
    - **[Catastrophic Misuse](scenario-catastrophic-misuse.md)** — Powerful capabilities enable sophisticated harm

    **Why this matters:** Even if technical safety problems are fully solved, Australia still needs governance, resilience, and democratic participation in AGI transformation. Alignment is necessary but not sufficient.

---

## Focus on Capabilities, Not Definitions

A common trap in AI strategy is getting stuck debating the definition of "AGI."

**Strategic approach:** We do not need to agree on a precise definition to prepare for the risks. Instead, we focus on **capabilities**—what systems can actually *do*.

As the [RAND Corporation](https://geopoliticsagi.substack.com/p/taking-agi-seriously-not-literally) notes, "the significance of AGI is that specific capabilities could come 'for free' with AGI—without needing to devote effort to developing a specialised AI."

This means preparing for a security environment where:

*   **Capabilities arrive effectively "for free":** As general reasoning improves, systems gain specialised skills (like cyber-offense or persuasion) without anyone explicitly training them for that purpose.
*   **Access lowers barriers:** Advanced capabilities that previously required nation-state resources become accessible to smaller actors.

The scenarios below focus on these **capability shifts** and their impact on Australia, rather than predicting a specific "AGI moment."

---

## Scenarios at a glance

<div class="grid cards" markdown>

-   :lucide-chart-no-axes-combined:{ .lg .middle } **[Power Concentration & Governance Failure](scenario-power-concentration.md)**

    ---

    **C·A·G·R focus:** [Governance](../framework/governance.md), [Containment](../framework/containment.md), [Resilience](../framework/resilience.md)

    **Timeline:** Near-term (underway)

    **Key vulnerability:** Heavy reliance on imported tech; regulatory race to the bottom

-   :lucide-trending-down:{ .lg .middle } **[Gradual Disempowerment](scenario-gradual-disempowerment.md)**

    ---

    **C·A·G·R focus:** [Resilience](../framework/resilience.md), [Governance](../framework/governance.md)

    **Timeline:** Near to medium-term

    **Key vulnerability:** Efficiency pressures; skills atrophy through automation

-   :lucide-globe:{ .lg .middle } **[Democracy & Information Ecosystems](scenario-information-ecosystems.md)**

    ---

    **C·A·G·R focus:** [Resilience](../framework/resilience.md), [Governance](../framework/governance.md)

    **Timeline:** Near-term (underway)

    **Key vulnerability:** Small media market; high social media dependence

-   :lucide-zap:{ .lg .middle } **[Critical Infrastructure Cascade](scenario-critical-infrastructure.md)**

    ---

    **C·A·G·R focus:** [All four pillars](../framework/index.md)

    **Timeline:** Medium-term

    **Key vulnerability:** Geographic isolation; optimised systems without redundancy

-   :lucide-shield-alert:{ .lg .middle } **[Catastrophic Misuse](scenario-catastrophic-misuse.md)**

    ---

    **C·A·G·R focus:** [Containment](../framework/containment.md), [Resilience](../framework/resilience.md)

    **Timeline:** Medium to longer-term

    **Key vulnerability:** Limited domestic defence R&D; dependence on partners

-   :lucide-cpu:{ .lg .middle } **[Loss of Control](scenario-loss-of-control.md)**

    ---

    **C·A·G·R focus:** [Alignment](../framework/alignment.md), [Containment](../framework/containment.md), [Resilience](../framework/resilience.md)

    **Timeline:** Longer-term (higher uncertainty)

    **Key vulnerability:** Limited capability to evaluate or roll back frontier systems

</div>

---

## How scenarios connect: compounding effects

Real crises rarely follow a single scenario cleanly. Multiple threat pathways often activate simultaneously, creating compounding effects:

**Power concentration + Information ecosystem strain**

- Small number of providers control critical systems AND the platforms that mediate public information
- During crisis, platforms make content moderation decisions that shape public response
- Government has limited leverage because alternative infrastructure doesn't exist
- Result: Much harder to maintain information integrity and democratic accountability

**Gradual disempowerment + Critical infrastructure failure**

- Institutions have lost manual operation skills over years of automation
- Critical infrastructure systems fail or are attacked
- Staff can't revert to manual operation quickly enough
- Result: Extended service disruption, much slower recovery

**Information ecosystem degradation + Any other scenario**

- When trust in information is low, coordinating response to any other crisis becomes much harder
- Misinformation and confusion compound every other problem

**For planning:** When running exercises, deliberately combine 2-3 scenarios to explore how your responses hold up under compounding stress.

---

## What makes Australia particularly vulnerable (and resilient)

Understanding Australia's specific context helps explain why certain scenarios pose greater risks here than elsewhere—and what strengths we can build on.

**Particular vulnerabilities:**

- Small population and limited domestic AI capability
- Geographic isolation increases supply chain brittleness
- Heavy reliance on imported technology and platforms
- Critical infrastructure increasingly AI-mediated
- Small number of key providers across many sectors

**Particular strengths:**

- Strong institutions and regulatory traditions
- High trust in government (relatively speaking)
- Close security partnerships providing intelligence and capability sharing
- Strong community networks, especially outside major cities
- History of adapting to external shocks (fires, floods, pandemics)

Effective strategy builds on our strengths while directly addressing our vulnerabilities. The scenarios below show how these vulnerabilities could be exploited—and where our strengths provide opportunity for resilience.

---

## Why these scenarios matter regardless of timeline uncertainty

### AGI timelines remain uncertain—preparation cannot wait

A common objection: "If we don't know when AGI will arrive, why act now?"

**The core argument:**

Leading AI researchers emphasise that **the case for preparedness doesn't depend on predicting exact timelines**. Whether advanced AI systems arrive in 5, 15, or 30 years, several things remain true:

1. **Capability building takes time:** The institutional expertise, evaluation frameworks, governance structures, and response capabilities needed to manage advanced AI risks take years to develop. Waiting for certainty means being unprepared.

2. **The plausibility threshold is already crossed:** Expert opinion across the AI field—including Nobel and Turing Prize winners like Geoffrey Hinton and Yoshua Bengio—assesses that transformative AI within 5-30 years is plausible enough to warrant serious preparation. As Berkeley professor Stuart Russell argues: if this outcome is plausible within roughly 30 years, "that's a huge fucking deal" requiring action now.

3. **Expectations have already compressed significantly:** Between 2022-2024, expert estimates for advanced AI shifted from "many decades away" to "potentially within the next few years to 20 years"—a dramatic compression even without AGI actually arriving. This shift alone justifies policy attention.

4. **Hype cycles don't eliminate risks:** The 2025 "AI hype correction"—triggered by slower-than-expected progress from models like GPT-5—has led some to dismiss safety concerns as overblown. But capability progress isn't linear, and safety frameworks must be resilient to both hype and backlash cycles.

5. **Preparation is valuable across scenarios:** The capabilities needed to manage AGI risks—evaluation expertise, AI control methods, incident response systems, international coordination—are also valuable for managing risks from less capable systems deployed today.

### What this means for scenario planning

When working with these scenarios:

- **Don't fixate on exact dates:** The scenarios include indicative timelines ("near-term," "medium-term," "longer-term") to help with prioritisation, but the specific pathways and vulnerabilities matter more than precise timing.

- **Focus on plausibility, not probability:** You don't need high confidence that a scenario will happen to justify preparation—you need sufficient plausibility that the scenario *could* happen within policy-relevant timeframes.

- **Build capabilities that scale:** The best preparedness investments are those that help with both near-term risks and longer-term scenarios.

- **Plan for the time it takes:** Australia's geographic isolation, small domestic AI industry, and institutional capability gaps mean we need *more* lead time than jurisdictions with established AI sectors—another reason to start now.

**Bottom line:** Uncertainty about AGI timelines is not a reason to delay preparation—it's a reason to start building robust, flexible capabilities now that will remain valuable across multiple scenarios and timelines.

---

## Which scenario should you start with?

=== ":material-bank: Government & Public Institutions"

    Start with [Power Concentration](scenario-power-concentration.md) and [Critical Infrastructure](scenario-critical-infrastructure.md)

=== ":material-briefcase: Business & Industry"

    Start with [Gradual Disempowerment](scenario-gradual-disempowerment.md) and [Power Concentration](scenario-power-concentration.md)

=== ":material-account-group: Communities & Households"

    Start with [Information Ecosystems](scenario-information-ecosystems.md) and [Critical Infrastructure](scenario-critical-infrastructure.md)

=== ":lucide-alert-triangle: Planning for high-impact events"

    Start with [Catastrophic Misuse](scenario-catastrophic-misuse.md) and [Loss of Control](scenario-loss-of-control.md)

---

## Next steps

**Just starting?**
Pick one scenario from the table above. Read it. Answer the questions for your actor type. That's enough to be useful.

**Ready for deeper work?**
Run a structured tabletop exercise with your team. The scenario pages include specific questions to guide discussion.

**Looking for response options?**
The [Containment · Alignment · Governance · Resilience](../framework/index.md) framework organises what government, business and communities can do.

**Need sector-specific guidance?**
- [Government & Policy](../government-policy/index.md)
- [Business & Industry](../business-industry/index.md)
- [Communities & Households](../communities-households/index.md)

---

??? note "Sources & Further Reading"

    **Australian context:**

    - Australian Strategic Policy Institute [Critical Technology Tracker](https://www.aspi.org.au/report/critical-technology-tracker)
    - [Royal Commission into the Robodebt Scheme](https://robodebt.royalcommission.gov.au/) (2023)
    - Australian Government [National AI Centre](https://www.csiro.au/en/about/challenges-missions/naic)
    - Department of Defence [Critical Technologies List](https://www.defence.gov.au/about/reviews-inquiries/critical-technologies)
    - Australian Cyber Security Centre [Critical Infrastructure Security](https://www.cyber.gov.au/)

    **Foundational AI safety research:**

    - Russell (2019) *Human Compatible: AI and the Problem of Control*
    - Bostrom (2014) *Superintelligence: Paths, Dangers, Strategies*
    - Brundage et al. (2018) ["The Malicious Use of Artificial Intelligence"](https://arxiv.org/abs/1802.07228)
    - Ngo et al. (2022) ["The alignment problem from a deep learning perspective"](https://arxiv.org/abs/2209.00626)

    **International governance:**

    - OECD [AI Policy Observatory](https://oecd.ai/)
    - UK AI Safety Institute [international collaboration](https://www.aisi.gov.uk/)
    - Centre for the Governance of AI [research library](https://www.governance.ai/)


