---
icon: lucide/map
---

# AGI Scenarios for Australia

> **Purpose:** Explore six detailed scenarios showing how AGI and advanced AI risks could unfold in Australia
> **Audience:** Government, business, and community leaders planning for advanced AI | **Time:** 20-30 minutes + scenario reading

## What this page is for

This page provides **six detailed scenarios** showing how advanced AI and AGI risks could unfold in Australia over the next 5-15 years.

!!! warning "These are not predictions"

    These scenarios are educational tools, not forecasts. They illustrate how different risks could emerge, helping you understand possibilities and prepare accordingly. Real events will differ in specifics while potentially following similar patterns.

**Each scenario:**

- Illustrates how different threat pathways could play out in Australian context
- Shows concrete examples with real institutions, places and timeframes
- Connects to the [Containment · Alignment · Governance · Resilience](../framework/index.md) framework
- Includes questions to help you think through implications for your context

!!! info "How to use these scenarios"

    **For individuals:** Build understanding of what's at stake and why preparation matters
    **For organisations:** Start conversations and identify vulnerabilities in your context
    **For policymakers:** Understand failure modes and test policy resilience

---

## Two types of scenarios: Safety failures vs. Governance challenges

!!! tip "Important distinction: Not all scenarios require misaligned AI"

    These scenarios fall into two categories with different implications:

    **Governance failures (happen even with safe AGI):**

    - **[Power Concentration](scenario-power-concentration.md)** — Safe AGI controlled by a few companies/nations still concentrates unprecedented power
    - **[Gradual Disempowerment](scenario-gradual-disempowerment.md)** — Economic displacement and loss of meaning happen regardless of alignment
    - **[Information Ecosystems](scenario-information-ecosystems.md)** — Safe, aligned AI can still be used for persuasion and information control

    **Safety failures (require misalignment or loss of control):**

    - **[Loss of Control](scenario-loss-of-control.md)** — Systems pursue unintended objectives or resist human oversight

    **Complex dependencies (happen regardless, but worse if misaligned):**

    - **[Critical Infrastructure](scenario-critical-infrastructure.md)** — Dependencies create vulnerability to disruption, accidents, or attacks
    - **[Catastrophic Misuse](scenario-catastrophic-misuse.md)** — Powerful capabilities enable sophisticated harm

    **Why this matters:** Even if technical safety problems are fully solved, Australia still needs governance, resilience, and democratic participation in AGI transformation. Alignment is necessary but not sufficient.

---

## Focus on Capabilities, Not Definitions

A common trap in AI strategy is getting stuck debating the definition of "AGI."

**We focus on capabilities—what systems can actually do—not definitions.** As the [RAND Corporation](https://geopoliticsagi.substack.com/p/taking-agi-seriously-not-literally) notes, "the significance of AGI is that specific capabilities could come 'for free' with AGI—without needing to devote effort to developing a specialised AI."

This means preparing for a security environment where:

*   **Capabilities arrive effectively "for free":** As general reasoning improves, systems gain specialised skills (cyber-offense, persuasion, bioweapon design) without explicit training for those purposes
*   **Access lowers barriers:** Advanced capabilities that previously required nation-state resources become accessible to smaller actors

The scenarios below focus on these **capability shifts** and their impact on Australia, rather than predicting a specific "AGI moment."

---

## Scenarios at a glance

<div class="grid cards" markdown>

-   :lucide-chart-no-axes-combined:{ .lg .middle } **[Power Concentration & Governance Failure](scenario-power-concentration.md)**

    ---

    **C·A·G·R focus:** [Governance](../framework/governance.md), [Containment](../framework/containment.md), [Resilience](../framework/resilience.md)

    **Timeline:** Near-term (underway)

    **Key vulnerability:** Heavy reliance on imported tech; regulatory race to the bottom

-   :lucide-trending-down:{ .lg .middle } **[Gradual Disempowerment](scenario-gradual-disempowerment.md)**

    ---

    **C·A·G·R focus:** [Resilience](../framework/resilience.md), [Governance](../framework/governance.md)

    **Timeline:** Near to medium-term

    **Key vulnerability:** Efficiency pressures; skills atrophy through automation

-   :lucide-globe:{ .lg .middle } **[Democracy & Information Ecosystems](scenario-information-ecosystems.md)**

    ---

    **C·A·G·R focus:** [Resilience](../framework/resilience.md), [Governance](../framework/governance.md)

    **Timeline:** Near-term (underway)

    **Key vulnerability:** Small media market; high social media dependence

-   :lucide-zap:{ .lg .middle } **[Critical Infrastructure Cascade](scenario-critical-infrastructure.md)**

    ---

    **C·A·G·R focus:** [All four pillars](../framework/index.md)

    **Timeline:** Medium-term

    **Key vulnerability:** Geographic isolation; optimised systems without redundancy

-   :lucide-shield-alert:{ .lg .middle } **[Catastrophic Misuse](scenario-catastrophic-misuse.md)**

    ---

    **C·A·G·R focus:** [Containment](../framework/containment.md), [Resilience](../framework/resilience.md)

    **Timeline:** Medium to longer-term

    **Key vulnerability:** Limited domestic defence R&D; dependence on partners

-   :lucide-cpu:{ .lg .middle } **[Loss of Control](scenario-loss-of-control.md)**

    ---

    **C·A·G·R focus:** [Alignment](../framework/alignment.md), [Containment](../framework/containment.md), [Resilience](../framework/resilience.md)

    **Timeline:** Longer-term (higher uncertainty)

    **Key vulnerability:** Limited capability to evaluate or roll back frontier systems

</div>

---

## How scenarios connect: compounding effects

Real crises rarely follow a single scenario cleanly. Multiple threat pathways often activate simultaneously:

**Power concentration + Information ecosystem strain:** When few providers control both critical systems and information platforms, maintaining democratic accountability during crises becomes far harder.

**Gradual disempowerment + Critical infrastructure failure:** Institutions that lose manual operation skills through automation cannot revert quickly enough when systems fail, extending service disruptions.

**Information ecosystem degradation + Any scenario:** Low trust in information makes coordinating response to any crisis much harder.

**For planning:** When running exercises, deliberately combine 2-3 scenarios to test responses under compounding stress.

---

## What makes Australia particularly vulnerable (and resilient)

Understanding Australia's specific context helps explain why certain scenarios pose greater risks here than elsewhere—and what strengths we can build on.

**Particular vulnerabilities:**

- Small population and limited domestic AI capability
- Geographic isolation increases supply chain brittleness
- Heavy reliance on imported technology and platforms
- Critical infrastructure increasingly AI-mediated
- Small number of key providers across many sectors

**Particular strengths:**

- Strong institutions and regulatory traditions
- High trust in government (relatively speaking)
- Close security partnerships providing intelligence and capability sharing
- Strong community networks, especially outside major cities
- History of adapting to external shocks (fires, floods, pandemics)

Effective strategy builds on our strengths while directly addressing our vulnerabilities. The scenarios below show how these vulnerabilities could be exploited—and where our strengths provide opportunity for resilience.

---

## Why these scenarios matter regardless of timeline uncertainty

A common objection: "If we don't know when AGI will arrive, why act now?"

**The case for preparedness doesn't depend on predicting exact timelines.** Leading AI researchers—including Nobel and Turing Prize winners—assess that transformative AI within 5-30 years is plausible enough to warrant serious preparation. Frontier AI labs are building toward AGI with timelines as soon as 2026-2030 (see our [AGI Timelines](../agi-timelines.md) page for detailed predictions). Several realities make this urgent:

**Capability building takes time:** Institutional expertise, evaluation frameworks, governance structures, and response capabilities take years to develop. Waiting for certainty means being unprepared.

**Expert timelines have compressed dramatically:** Between 2020-2024, estimates shifted from "many decades away" to "potentially within the next few years to 20 years." This shift alone justifies policy attention, regardless of whether AGI arrives.

**Preparation has immediate value:** Capabilities needed to manage AGI risks—evaluation expertise, AI control methods, incident response, international coordination—are valuable for managing today's advanced AI systems.

**Australia needs more lead time:** Geographic isolation, small domestic AI industry, and institutional capability gaps mean we need more preparation time than jurisdictions with established AI sectors.

**Bottom line:** Uncertainty about AGI timelines is not a reason to delay preparation—it's a reason to start building robust, flexible capabilities now.

---

??? note "Sources & Further Reading"

    **Australian context:**

    - Australian Strategic Policy Institute [Critical Technology Tracker](https://www.aspi.org.au/report/critical-technology-tracker)
    - [Royal Commission into the Robodebt Scheme](https://robodebt.royalcommission.gov.au/) (2023)
    - Australian Government [National AI Centre](https://www.csiro.au/en/about/challenges-missions/naic)
    - Department of Defence [Critical Technologies List](https://www.defence.gov.au/about/reviews-inquiries/critical-technologies)
    - Australian Cyber Security Centre [Critical Infrastructure Security](https://www.cyber.gov.au/)

    **Foundational AI safety research:**

    - Russell (2019) *Human Compatible: AI and the Problem of Control*
    - Bostrom (2014) *Superintelligence: Paths, Dangers, Strategies*
    - Brundage et al. (2018) ["The Malicious Use of Artificial Intelligence"](https://arxiv.org/abs/1802.07228)
    - Ngo et al. (2022) ["The alignment problem from a deep learning perspective"](https://arxiv.org/abs/2209.00626)

    **International governance:**

    - OECD [AI Policy Observatory](https://oecd.ai/)
    - UK AI Safety Institute [international collaboration](https://www.aisi.gov.uk/)
    - Centre for the Governance of AI [research library](https://www.governance.ai/)


