---
icon: lucide/trending-down
tags:
  - Scenario
  - Governance
  - Alignment
  - Democracy
---

# Scenario 2: AI Over-Reliance and Institutional Disempowerment

![AI Over-Reliance and Institutional Disempowerment](scenario-gradual-disempowerment-hero.jpg)

## Summary

In 2026, Services Australia deploys an AI system to assess welfare eligibility and payment amounts. Initial results are promising: faster processing, fewer errors, more consistent decisions. By 2028, 85% of Centrelink decisions are AI-recommended, with human case workers reviewing only flagged cases.

Meanwhile, major banks adopt AI credit assessment systems. NAB, CommBank and Westpac all license similar models from the same US provider. Loan officers can see the AI's decision but rarely understand its reasoning. Overruling the system triggers compliance reviews. By 2029, manual overrides drop to less than 2%.

State health departments follow suit: AI triage in emergency departments, AI-assisted diagnosis in Medicare-funded consultations, algorithmic resource allocation during the 2030 flu season. In each case, the promise is efficiency and consistency.

**The erosion is gradual.** By 2033, when a bank's AI system denies mortgages to an entire postcode without explanation, nobody on staff can articulate why. When a hospital's resource allocation algorithm prioritises certain demographics during a crisis, the review finds that current staff don't understand the system well enough to identify the bias—and the vendor's technical team is overseas.

Citizens trying to appeal decisions face Kafka-esque loops: "the system determined..." with no human who can explain or override it. Skills have atrophed. The ability to operate manually—even in a crisis—has been lost.

!!! warning "This is a scenario, not a prediction"

    This scenario illustrates how gradual automation can erode human capability over time. Real implementations may unfold differently, but the pattern—incremental dependency leading to skills atrophy—is already observable in current automated decision systems.

!!! info "Threat pathways"
    This scenario illustrates two interconnected pathways:

    **Gradual disempowerment** – Incremental automation causes skills atrophy and loss of manual operation capability

    **Power concentration** – Dependencies on vendor systems create lock-in; accountability becomes obscured across complex supply chains

---

## Implications for C·A·G·R Framework

This scenario explores how incremental automation erodes human capability and institutional resilience over time. The gradual nature makes it difficult to reverse, testing whether organisations can maintain essential functions when AI dependencies become too deep.

=== ":lucide-shield-ban: Containment"

    - No dramatic failure that triggers containment measures
    - Gradual expansion of AI decision-making doesn't cross obvious red lines
    - By the time dependency becomes obvious, rolling back is extremely difficult
    - Individual deployments seem reasonable; aggregate effect is disempowerment

=== ":lucide-target: Alignment"

    - Systems may be nominally "aligned" with policy objectives, but real-world values drift may go unnoticed
    - Feedback loops favour optimisation for efficiency and cost over fairness, robustness or contestability
    - Misalignment manifests as systematic bias or injustice that compounds over time

=== ":lucide-scale: Governance"

    - Formal oversight remains, but in practice becomes weaker as human understanding and challenge capacity degrade
    - Accountability becomes blurred: "the system recommended it"
    - Regulatory frameworks struggle when the regulated can't explain their own decision processes
    - Approval processes fail to account for cumulative effects across many deployments

=== ":lucide-shield: Resilience"

    - In a crisis – or if systems exhibit unexpected behaviour – institutions may struggle to revert to manual processes
    - Citizens may feel increasingly powerless, undermining trust and social cohesion
    - Recovery from AI system failure takes much longer when human capability has atrophied
    - Skills and institutional knowledge needed for manual operation have eroded

---

## Questions for actors

Use these questions for risk assessments, strategic planning, and tabletop exercises.

=== ":material-bank: Government & Public Institutions"

    **Near-term (within 12 months):**

    - Pick one decision workflow that now involves AI. Walk through a real case: who actually has the power to overrule the AI? Has anyone done so in the past six months?
    - What skills are you actively maintaining even though AI can do those tasks? Are junior staff learning those skills?
    - If your primary AI system was unavailable for a week, how long would it take to restore full manual operation?
    - What proportion of staff could actually explain how key decisions are made, without referring to "the system"?

    **Strategic:**

    - Where are decisions effectively being delegated to AI, and what meaningful human oversight exists in practice?
    - How are contestability, redress and transparency protected as automation expands?
    - Do contingency plans exist for operating critical functions without these systems?

=== ":material-briefcase: Business & Industry"

    **Near-term (within 12 months):**

    - When was the last time a human overruled an AI recommendation in your organisation? Was that person rewarded or discouraged?
    - Are performance metrics designed to optimise AI efficiency, human oversight, or both?
    - What's your plan for maintaining human expertise in roles being automated?
    - Could key staff actually perform their jobs manually if systems failed?

    **Strategic:**

    - Are human staff empowered and trained to challenge AI outputs, or is challenge discouraged in practice?
    - How do your incentives (KPIs, performance metrics) shape the balance between efficiency and oversight?
    - Could your organisation continue to function if key AI systems were temporarily unavailable?

=== ":material-account-group: Communities & Households"

    **Near-term (within 12 months):**

    - How easy is it to appeal an AI-mediated decision that affects you (credit denial, benefits assessment, job rejection)?
    - Have you or someone you know tried? What happened?
    - What community resources exist to help people who can't navigate automated systems?
    - Which local organisations still maintain human decision-making capacity?

    **Strategic:**

    - How easy is it for individuals to understand or challenge AI-mediated decisions that affect them?
    - What civil society or community support exists to help people navigate these systems?
    - Which decisions should never be fully automated, even if AI could handle them efficiently?

---

!!! question "Isn't automation just efficiency? Why is this a problem?"

    **Efficiency gains are real—but they come with trade-offs:**

    - Individual automation decisions improve productivity
    - But aggregate dependency creates vulnerability when systems fail
    - Skills atrophy means you can't revert to manual operation quickly
    - Accountability erodes when "the system decided" becomes the default answer

    **The problem isn't automation itself—it's automation without:**

    - Maintaining parallel human capability
    - Testing manual fallbacks under realistic conditions
    - Preserving meaningful human oversight and contestability
    - Governing cumulative effects across many deployments

    This scenario shows that relying entirely on efficiency optimization without resilience planning creates systemic risk.

---

## Why this scenario matters for Resilience and Governance

This scenario illustrates **the frog-in-boiling-water problem**—gradual changes that never trigger containment responses but accumulate to create systemic vulnerability.

**The challenge for resilience:**

- Skills atrophy is silent and incremental
- Loss of manual operation capability isn't noticed until crisis
- By the time dependency is obvious, rolling back causes disruption

**The challenge for governance:**

- Individual automation decisions seem reasonable
- Aggregate effects across many deployments aren't governed
- Accountability erodes ("the system decided") without obvious violations
- Contestability and redress mechanisms degrade slowly

**Key insights:**

1. **Prevention requires foresight:** Must intervene before dependency becomes irreversible
2. **Maintain parallel capability:** Skills and manual operation must be actively preserved
3. **Test under stress:** Can you actually operate manually, or is it theoretical?
4. **Governance must address cumulative effects:** Not just individual deployments

**Use this scenario to:**

- Identify where your organisation has become too dependent on AI systems
- Test whether manual operation is realistic or just on paper
- Design governance that addresses cumulative automation effects
- Plan for maintaining human capability alongside automation

---

??? note "Sources & Further Reading"
    This scenario draws from research on algorithmic governance, automation of public services, and real-world precedents of over-reliance on automated decision systems.

    **Australian precedents:** [Royal Commission into the Robodebt Scheme](https://robodebt.royalcommission.gov.au/) (2023) · [Australian Government Digital Transformation Agency](https://www.dta.gov.au/help-and-advice/about-digital-identity/identity-data-and-privacy-principles) automated decision-making guidelines

    **Academic research:** Eubanks (2018) *Automating Inequality* · Yeung (2018) ["Algorithmic regulation"](https://doi.org/10.1111/rego.12158) · Pasquale (2015) *The Black Box Society*

    **Policy organisations:** [AI Now Institute](https://ainowinstitute.org/) · [Australian Human Rights Commission](https://humanrights.gov.au/our-work/technology-and-human-rights) · [AlgorithmWatch](https://algorithmwatch.org/)

    **Case studies:** UK Post Office Horizon scandal · Netherlands childcare benefits scandal (2019-2021) · Allegheny Family Screening Tool (US)

    **Key concepts:** See our [Concepts & Glossary](../concepts.md) for definitions of automation bias, algorithmic accountability, explainability and human-in-the-loop systems
