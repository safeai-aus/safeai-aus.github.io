---
icon: lucide/shield-check
title: "Voluntary AI Safety Standard (10 Guardrails)"
description: "Comprehensive guide to Australia's Voluntary AI Safety Standard with 10 practical guardrails for safe AI adoption. Includes implementation guidance for SMEs and large organisations."
keywords: "Australian AI safety standard, AI guardrails, AI safety framework, voluntary AI standards, AI risk management, AI governance Australia, AI safety compliance, AI safety guardrails, Australian AI standards, AI safety best practices"
author: "SafeAI-Aus"
robots: "index, follow"
og_title: "Voluntary AI Safety Standard (10 Guardrails)"
og_description: "Comprehensive guide to Australia's Voluntary AI Safety Standard with 10 practical guardrails"
og_type: "article"
og_url: "https://safeaiaus.org/safety-standards/voluntary-ai-safety-standard-10-guardrails/"
og_image: "assets/safeaiaus-logo-600px.png"
twitter_card: "summary_large_image"
twitter_title: "Voluntary AI Safety Standard (10 Guardrails)"
twitter_description: "Comprehensive guide to Australia's Voluntary AI Safety Standard with 10 practical guardrails"
---

# Voluntary AI Safety Standard (10 Guardrails)

> **Purpose:** Detailed control catalogue for implementing Australia's 10 AI safety guardrails
> **Audience:** Governance, risk, compliance and technical teams | **Time:** 45-60 minutes for full review

Australia's **Voluntary AI Safety Standard (VAISS)** provides ten practical guardrails organisations can adopt now to deploy and use AI safely and responsibly.  

The standard was developed by the Department of Industry, Science and Resources (DISR) and first published in November 2023. In 2024, the guardrails were updated following consultation to align more closely with the Government‚Äôs proposed mandatory guardrails for high-risk AI applications.  

The standard is **voluntary** and complements existing Australian law. It provides a practical framework for organisations to manage AI safely while future regulation is considered.  

Importantly, the 10 guardrails are consistent with leading international standards and frameworks, including:

- [**ISO/IEC 42001:2023** ‚Äì AI Management System Standard](https://www.iso.org/standard/81230.html)
- [**NIST AI Risk Management Framework 1.0**](https://www.nist.gov/itl/ai-risk-management-framework)

!!! info "Status Update (October 2025)"
    In October 2025 the National AI Centre released the **Guidance for AI Adoption**, which sets out 6 essential practices (AI6) for responsible AI governance and adoption. This guidance is described as the **first update of the Voluntary AI Safety Standard (VAISS)** and is now the primary reference for organisations using AI in Australia.

    The 10 guardrails on this page remain fully integrated into the new guidance and are best used as a **detailed control set** and historical reference, especially where contracts, risk registers or external frameworks still refer to the original VAISS guardrails.

## Why this matters

Adopting the guardrails early helps organisations build **trust, resilience and regulatory readiness**. By embedding these practices now, businesses can:

- üõ°Ô∏è Reduce risks from bias, errors and misuse of AI
- ü§ù Strengthen transparency and customer confidence
- üöÄ Position themselves ahead of future mandatory compliance requirements
- ‚≠ê Demonstrate leadership in responsible AI adoption  

## The 10 Guardrails
1. Establish, implement and publish an accountability process
2. Establish and implement a risk management process
3. Protect AI systems and implement data governance measures
4. Test AI models and systems
5. Enable human control or intervention
6. Inform end-users regarding AI-enabled decisions
7. Establish processes for people to challenge use or outcomes
8. Be transparent with other organisations across the AI supply chain
9. Keep and maintain records
10. Engage your stakeholders and evaluate their needs  

!!! success "What the guardrails do"
    - ‚úÖ Encourage **transparency and accountability** for AI systems
    - ‚úÖ Require **risk assessment, testing and human oversight** before and after deployment
    - ‚úÖ Promote **record-keeping** and **supplier due-diligence** across the AI supply chain
    - ‚úÖ Emphasise **stakeholder engagement** and ongoing monitoring as systems evolve

## How this maps to the 6 essential practices (AI6)

The Guidance for AI Adoption condenses the 10 VAISS guardrails into 6 essential practices. At a high level:

| AI6 practice                         | Closest VAISS guardrails and themes                            |
|-------------------------------------|-----------------------------------------------------------------|
| Decide who is accountable           | 1. Establish accountability; 9. Maintain records                |
| Understand impacts and plan accordingly | 2. Implement risk management; 7. Monitor impacts; 10. Support human autonomy |
| Measure and manage risks            | 2. Implement risk management; 3. Protect data                   |
| Share essential information         | 4. Ensure transparency; 8. Ensure accountability in the supply chain; elements of 7 and 10 |
| Test and monitor                    | 6. Test reliability; 7. Monitor impacts                         |
| Maintain human control              | 5. Enable human control; 10. Support human autonomy             |

!!! tip "How to use this mapping"
    - üìä Use **AI6** as your top-level framework when explaining AI governance to boards, executives and regulators
    - üìã Use the **10 guardrails** in this page as a **control library** when:
        - building or updating AI policies and standards
        - designing AI risk assessments and risk registers
        - writing contractual clauses and supplier questionnaires
    - üîó Where you see references to "VAISS" or "guardrails" in external documents, you can confidently interpret them through this AI6 lens

!!! tip "How to use this in your business"
    1. ‚úÖ Adopt the 10 guardrails as acceptance criteria for any AI initiative
    2. üìù Update policies and procurement to reflect supplier alignment with the guardrails
    3. üîÑ Integrate testing, documentation and oversight into your normal change-management
    4. üìÖ Review systems at least annually or on material change

---

## SME-Scaled Implementation Approach

While the 10 guardrails apply to all organisations, SMEs can adopt them at different maturity levels:

**Guardrail 1: Establish, implement and publish an accountability process** 

- *Minimum:* Designate an AI responsible person  
- *Better:* Create simple AI governance policy  
- *Best:* Regular board/leadership AI updates  

**Guardrail 2: Establish and implement a risk management process**  

- *Minimum:* Use SAAM risk assessment tool  
- *Better:* Quarterly risk reviews  
- *Best:* Integrated risk management system  

**Guardrail 3: Protect AI systems and implement data governance measures** 

- *Minimum:* Follow existing cybersecurity practices  
- *Better:* AI-specific data controls  
- *Best:* Enhanced encryption and access controls  

**Guardrail 4: Test AI models and systems**

- *Minimum:* Pre-deployment testing
- *Better:* Monthly performance monitoring
- *Best:* Continuous testing and validation  

**Guardrail 5: Enable human control or intervention** 

- *Minimum:* Override capability for all AI decisions  
- *Better:* Human review of significant decisions  
- *Best:* Human-in-the-loop for all critical processes  

**Guardrail 6: Inform end-users regarding AI-enabled decisions**

- *Minimum:* "Powered by AI" labels
- *Better:* Explain AI role in decisions
- *Best:* Full algorithmic transparency  

**Guardrail 7: Establish processes for people to challenge use or outcomes**

- *Minimum:* Complaint email or form
- *Better:* Documented appeal process
- *Best:* Independent review mechanism  

**Guardrail 8: Be transparent with other organisations across the AI supply chain**

- *Minimum:* Vendor compliance check  
- *Better:* Contractual AI requirements  
- *Best:* Regular vendor audits  

**Guardrail 9: Keep and maintain records**

- *Minimum:* Keep AI decision logs  
- *Better:* Comprehensive documentation  
- *Best:* Automated compliance reporting  

**Guardrail 10: Engage your stakeholders and evaluate their needs**

- *Minimum:* Stakeholder consultation at design
- *Better:* Regular stakeholder feedback loops
- *Best:* Co-design with diverse stakeholder groups  

---

## Summary Table

| Guardrail                        | Minimum Requirement                        | Better Practice                         | Best Practice                                      |
|----------------------------------|--------------------------------------------|-----------------------------------------|---------------------------------------------------|
| **1. Establish accountability**  | Designate responsible person                | Simple governance policy                 | Regular board/leadership AI updates                |
| **2. Risk management**           | Use SAAM risk tool                         | Quarterly reviews                        | Integrated risk management system                  |
| **3. Protect data**              | Follow cybersecurity basics                 | AI-specific controls                     | Enhanced encryption & access controls              |
| **4. Transparency**              | ‚ÄúPowered by AI‚Äù labels                      | Explain role in decisions                | Full algorithmic transparency                      |
| **5. Human control**             | Override capability                         | Human review of major decisions          | Human-in-the-loop for critical processes           |
| **6. Reliability testing**       | Pre-deployment testing                      | Monthly monitoring                       | Continuous testing & validation                    |
| **7. Monitor impacts**           | Track errors & complaints                   | Proactive assessments                    | Real-time monitoring dashboards                    |
| **8. Supply chain accountability** | Vendor compliance check                   | Contractual AI requirements              | Regular vendor audits                              |
| **9. Maintain records**          | Keep decision logs                          | Comprehensive documentation              | Automated compliance reporting                     |
| **10. Human autonomy**           | Opt-out options                             | User control preferences                 | Full user agency over interactions                 |

---

## Regulatory outlook and the future of the guardrails

As at December 2025, the position on AI-specific regulation in Australia has shifted.

The **National AI Plan** confirms that the Government will, for now, rely on **existing technology-neutral laws and sector regulators** (for example, privacy, consumer law, financial services, safety, anti-discrimination) to manage AI-related harms. Earlier proposals to introduce standalone mandatory guardrails and a dedicated AI Act for high-risk AI applications have been **paused**.

Instead:

- the **Guidance for AI Adoption (AI6)** provides voluntary but strongly endorsed governance expectations
- a new **AI Safety Institute** will be established from 2026 to monitor, test and advise on emerging AI capabilities, risks and harms
- regulators will continue to apply and, where necessary, adapt existing frameworks to AI use cases.

The 2024 consultation on mandatory guardrails and high-risk AI remains an important signal of how future regulation might be framed if the Government decides that existing laws are not sufficient. The principles-based definition of "high-risk AI" and the examples of sensitive domains (healthcare, employment, financial services, critical infrastructure, government decision-making, etc.) remain useful reference points for organisations when they assess and prioritise their own AI risk.

In practical terms, organisations should:

- treat the **10 guardrails + AI6** as **best-practice voluntary standards** rather than imminent hard law
- focus on integrating these controls into existing privacy, risk, safety, security and compliance processes
- monitor future updates from the AI Safety Institute, NAIC and sector regulators, which may influence how strongly certain guardrails are expected in particular industries.

### Government's principles-based definition of "high-risk AI"

In its earlier consultation on mandatory guardrails for high-risk settings, the Australian Government proposed a principles-based definition of "high-risk AI".

An AI application may be considered high-risk if it has a high likelihood of causing material harm in one or more of the following areas:

- human rights or freedoms
- health and safety
- legal rights or obligations
- democratic processes
- environmental outcomes
- broader societal impacts.

Risk was to be assessed based on context, severity and scale, rather than a static list of application types.

### Illustrative examples from public commentary

The following examples are not part of any official legal definition. They are drawn from media, legal analyses and the 2024 Senate inquiry into AI, which suggested that some systems and general-purpose models might warrant stronger oversight:

- healthcare diagnosis or treatment
- employment decisions (hiring, firing, promotion)
- financial services (loans, insurance, credit scoring)
- government service delivery and eligibility assessments
- critical infrastructure operation and security
- legal or quasi-legal decisions.

By contrast, many common "everyday" uses of AI (marketing automation, customer service chatbots, internal productivity tools, content generation and basic analytics) are generally treated as **lower-risk**, but must still comply with existing law and good-practice governance.

---

### Further Reading & Official Resources
- [Voluntary AI Safety Standard ‚Äì Overview and guardrails (Department of Industry)](https://www.industry.gov.au/publications/voluntary-ai-safety-standard)  
- [The 10 guardrails ‚Äì full guidance and examples (Department of Industry)](https://www.industry.gov.au/publications/voluntary-ai-safety-standard/10-guardrails)  
- [Legal landscape for AI in Australia (Department of Industry)](https://www.industry.gov.au/publications/voluntary-ai-safety-standard/legal-landscape-ai-australia)  
- [Consultation: Mandatory guardrails for high-risk settings (Australian Government)](https://consult.industry.gov.au/ai-mandatory-guardrails)  

---

<!-- JSON-LD: Article schema for crawlers and LLMs -->
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "Voluntary AI Safety Standard (10 Guardrails) - Australian AI Safety Framework",
  "description": "Comprehensive guide to Australia's Voluntary AI Safety Standard with 10 practical guardrails for safe AI adoption. Includes implementation guidance for SMEs and large organisations.",
  "author": {
    "@type": "Organization",
    "name": "SafeAI-Aus",
    "url": "https://safeaiaus.org"
  },
  "publisher": {
    "@type": "Organization",
    "name": "SafeAI-Aus",
    "url": "https://safeaiaus.org",
    "logo": {
      "@type": "ImageObject",
      "url": "https://safeaiaus.org/assets/safeaiaus-logo-600px.png"
    }
  },
  "datePublished": "2025-01-27",
  "dateModified": "2025-01-27",
  "inLanguage": "en-AU",
  "url": "https://safeaiaus.org/safety-standards/voluntary-ai-safety-standard-10-guardrails/",
  "mainEntityOfPage": "https://safeaiaus.org/safety-standards/voluntary-ai-safety-standard-10-guardrails/",
  "license": "https://creativecommons.org/licenses/by/4.0/",
  "isPartOf": {
    "@type": "WebSite",
    "name": "SafeAI-Aus",
    "url": "https://safeaiaus.org"
  },
  "about": [
    {
      "@type": "Thing",
      "name": "AI Safety Standards",
      "description": "Voluntary safety standards for AI implementation in Australia"
    },
    {
      "@type": "Thing",
      "name": "AI Guardrails",
      "description": "Practical principles for safe and responsible AI use"
    },
    {
      "@type": "Thing",
      "name": "Australian AI Governance",
      "description": "Governance frameworks for AI adoption in Australian businesses"
    }
  ],
  "keywords": "Australian AI safety standard, AI guardrails, AI safety framework, voluntary AI standards, AI risk management, AI governance Australia, AI safety compliance, AI safety guardrails, Australian AI standards, AI safety best practices",
  "articleSection": "Safety Standards",
  "wordCount": "2800"
}
</script>
