---
icon: lucide/clock
title: "AGI Timelines - When Could Advanced AI Arrive?"
description: "Expert predictions on AGI timelines vary dramatically. Understand the range of forecasts from frontier labs and AI safety researchers to inform Australian planning."
keywords: "AGI timeline, artificial general intelligence timeline, AI predictions, OpenAI predictions, Anthropic forecasts, DeepMind AGI, AI safety timeline Australia"
author: "SafeAI-Aus"
robots: "index, follow"
og_title: "AGI Timelines - When Could Advanced AI Arrive?"
og_description: "Expert predictions on AGI timelines vary dramatically. Understand forecasts from frontier labs and researchers."
og_type: "article"
og_url: "https://safeaiaus.org/agi-timelines/"
og_image: "assets/safeaiaus-logo-600px.png"
twitter_card: "summary_large_image"
twitter_title: "AGI Timelines - When Could Advanced AI Arrive?"
twitter_description: "Expert predictions on AGI timelines vary dramatically. Understand forecasts from frontier labs and researchers."
---

# AGI Timelines: When Could Advanced AI Arrive?

> **Purpose:** Understand expert predictions on AGI timelines and what they mean for Australian planning
> **Audience:** Government, business, and community leaders | **Time:** 15-20 minutes

**"When will we get AGI?"** is one of the most common questions about advanced AI—and one of the hardest to answer. Expert predictions vary dramatically, from "already here in narrow domains" to "decades away" to "may never happen."

!!! warning "Frontier labs are explicitly building toward AGI"

    This isn't theoretical speculation. OpenAI, Anthropic, DeepMind, and Chinese frontier labs are **actively developing systems intended to reach AGI**. Their timelines are compressed, their investment is massive (hundreds of billions of dollars), and their progress is accelerating. Whether they succeed on their timelines is uncertain—but the attempt is real and ongoing.

This page explains **why such disagreement exists**, what **frontier AI labs and thought leaders predict**, and **what this means for Australian planning**.

---

## What the experts say

### Frontier AI labs: Building toward AGI with aggressive timelines

**The organisations building the most advanced AI systems are explicitly racing toward AGI.** This isn't academic speculation—it's their stated mission, backed by hundreds of billions in investment and demonstrable capability gains every few months.

<div class="grid cards" markdown>

-   :material-rocket-launch:{ .lg .middle } **OpenAI (USA)**

    ---

    **Timeline:** AGI by **2027-2030**

    CEO Sam Altman has repeatedly stated OpenAI expects AGI-level capabilities "sooner than most people think." The company's mission statement is literally "ensure that artificial general intelligence benefits all of humanity." OpenAI's governance restructuring to a for-profit benefit corporation reflects planning for transformative AI arriving within this decade.

    **Key statement:** "The path to AGI should be short enough that we can see it clearly" — Sam Altman, 2023

-   :material-brain:{ .lg .middle } **Anthropic (USA)**

    ---

    **Timeline:** Transformative AI **2026-2028**

    CEO Dario Amodei expects "transformative AI" (systems that fundamentally change the world) under aggressive scaling scenarios within 2-4 years. In his essay ["Machines of Loving Grace"](https://darioamodei.com/machines-of-loving-grace), Amodei describes powerful AI arriving "by 2026 or 2027" and outlines the societal transformation this would bring.

    **Key statement:** "If we're right about the trajectory, we could see transformative AI as soon as 2026" — Dario Amodei, 2024

-   :material-google:{ .lg .middle } **DeepMind (Google, USA/UK)**

    ---

    **Timeline:** Human-level AI **2030-2040**

    DeepMind co-founder Shane Legg has maintained a **50% probability of AGI by 2028** since 2011. While Google is more cautious publicly, DeepMind's research roadmap and hiring patterns suggest internal planning for AGI-adjacent capabilities this decade. Their work on AlphaGo, AlphaFold, and Gemini demonstrates systematic capability scaling.

    **Key statement:** "50% confidence we'll have human-level AI by 2028" — Shane Legg, DeepMind co-founder

-   :material-flag:{ .lg .middle } **Chinese Frontier Labs**

    ---

    **Timeline:** AGI by **2030**

    China has made AGI a national strategic priority. Major frontier labs actively building toward AGI include **Baidu** (ERNIE models), **Alibaba** (Qwen/Tongyi Qianwen), **Tencent** (Hunyuan), **ByteDance** (Doubao), **Zhipu AI** (GLM models), and state-backed institutes including the **Beijing Academy of Artificial Intelligence** (Wu Dao) and **Shanghai AI Lab**. China's 2023 [Generative AI regulations](https://www.china-briefing.com/news/china-releases-generative-ai-regulations/) and massive compute investments reflect planning for transformative capabilities within this decade.

    **Key context:** China views AI as critical to technological sovereignty and military advantage, creating competitive pressure that accelerates global timelines

</div>

**These aren't passive predictions—they're active development roadmaps.** Frontier labs are scaling compute exponentially, hiring thousands of researchers, and iterating on architectures designed to reach general intelligence. Whether they'll succeed on their timelines is uncertain, but the race is real.

### Australian researchers and thought leaders

Several prominent Australian and Australia-connected researchers are shaping the AGI timeline conversation:

**Shane Legg** (New Zealand-born DeepMind co-founder and Chief AGI Scientist at Google DeepMind) has maintained a **50% probability of AGI by 2028** since 2009—over 15 years of consistent prediction. Legg coined the term "AGI" in 2002 and his timeline has remained remarkably stable. Recent [interviews](https://www.dwarkesh.com/p/shane-legg) reaffirm this forecast.

**Marcus Hutter** (Professor at Australian National University, senior researcher at DeepMind) is a world-leading theoretical AGI researcher known for developing **AIXI**, the mathematical framework for universal artificial intelligence. His work provides the theoretical foundations many AGI researchers build upon.

**Toby Walsh** (Scientia Professor at UNSW, Chief Scientist at UNSW.AI, Laureate Fellow at CSIRO Data61) conducted expert surveys predicting AGI around **2062**, representing more conservative academic timelines. Walsh serves on Australia's AI Expert Group advising government and is a prominent voice on AI ethics and governance.

**Helen Toner** (Melbourne-born, former OpenAI board member, Director at Georgetown CSET) provides the framework for understanding disagreement through her [three fundamental questions](https://helentoner.substack.com/p/unresolved-debates-about-the-future) about AI's future trajectory.

### International thought leaders and forecasting

**Leopold Aschenbrenner** (former OpenAI) argues [AGI by 2027](https://situational-awareness.ai/), becoming required reading in US defence circles. **Dario Amodei** (Anthropic CEO) describes [powerful AI by 2026-2027](https://www.darioamodei.com/essay/machines-of-loving-grace). **Sam Altman** (OpenAI CEO) expects [AGI this decade](https://blog.samaltman.com/the-gentle-singularity) but arriving gradually.

**Epoch AI** [literature reviews](https://epoch.ai/blog/literature-review-of-transformative-artificial-intelligence-timelines) show 57% probability by 2050 from expert aggregates. **Good Ancestors Policy** [survey](https://www.goodancestors.org.au/agi_survey_results) of 139 safety researchers found median 2035 timeline, 88.8% believing catastrophic risk is realistic.

### Broader academic surveys: Longer but compressing timelines

**AI Impacts** surveys (2022-2023) found median 2060 for high-level machine intelligence, though pre-GPT-4 results suggest researchers are updating toward shorter timelines. **Metaculus** aggregated forecasts show 50% by 2040-2045, compressed from 2050+ in 2020.

!!! info "Timeline compression is itself significant"

    Between 2020-2024, expert median timelines shortened by 10-20 years. This rapid compression—regardless of whether AGI actually arrives soon—warrants planning attention.

---

---

## Why such disagreement?

**Helen Toner** (Australian AI policy researcher, former OpenAI board member, Director of Strategy at Georgetown CSET) identifies [three fundamental unresolved questions](https://helentoner.substack.com/p/unresolved-debates-about-the-future) that determine whether—and how soon—transformative AI will arrive:

!!! question "Three fundamental questions shaping AGI timelines"

    **1. How far can we get with the current AI paradigm?** Will scaling current deep learning approaches reach AGI, or do we need fundamental breakthroughs?

    **2. How much can AI improve itself?** Will AI systems accelerate AI research (recursive improvement), or will progress remain bottlenecked by human oversight and empirical testing?

    **3. Will future AIs be tools we use, or agents that can act without us?** Does transformative AI require autonomous agents, or can tool-like AI still transform society?

    **Experts disagree sharply on all three questions—and the answers determine timelines.** If current paradigms scale, AI improves itself, and we deploy autonomous agents, AGI could arrive within years. If each requires breakthroughs, timelines extend to decades.

Beyond these three questions, five additional factors drive dramatic variation:

**Definition ambiguity:** "AGI" ranges from benchmark performance (arguably achieved) to economically transformative (automating most jobs) to true general intelligence (human-level across all domains) to superintelligence. Frontier labs use looser definitions; academics stricter ones.

**Scaling vs. breakthroughs:** Frontier labs believe current approaches will scale to AGI (more compute + data + algorithms). Academic sceptics argue fundamental breakthroughs are needed for robust reasoning, understanding, and transfer learning. Both could be right—scaled systems might automate jobs without solving all hard AI problems.

**Information and incentives:** Labs have privileged access to capabilities but incentives to appear cutting-edge. External researchers lack internal data but have less financial stake. Both see different parts: labs see rapid gains, researchers see persistent limitations.

**Compute trajectory uncertainty:** Timelines depend on continued exponential scaling of training compute (doubling every 6-12 months), investment (hundreds of billions), and algorithmic efficiency. Physical limits (chips, power), economic limits (diminishing returns), or regulatory limits could extend timelines.

**Recursive improvement wildcard:** If AI systems meaningfully accelerate AI research, progress could compress dramatically. If human oversight remains necessary, progress stays incremental. This is the widest uncertainty—recursive improvement could compress decades into years.

---

## What this means for Australian planning

**We can't know the exact timeline—but we can prepare across scenarios.** The responsible approach isn't betting on a single timeline, but building capabilities that work across multiple futures.

!!! tip "Planning principle: Prepare for the shortest plausible timeline while building flexible capacity"

    If frontier labs are wrong and AGI takes 40 years, Australia still benefits from AI evaluation expertise, governance frameworks, and resilience planning. If they're right and AGI arrives in 5-10 years, we can't afford to be unprepared. **Asymmetric risk favours preparation.**

<div class="grid cards" markdown>

-   :material-clock-fast:{ .lg .middle } **Short timelines (2027-2032)**

    ---

    **Probability:** 20-30% | **Impact:** Transformational

    If frontier labs are correct, Australia faces advanced AI transformation within one election cycle. This demands immediate capability building for AI evaluation and governance, regulatory frameworks in place before deployment, international coordination on safety standards, and strategic decisions about critical infrastructure dependencies.

    **Planning implication:** Build core capabilities now—evaluation teams, governance frameworks, international partnerships

-   :material-clock-outline:{ .lg .middle } **Medium timelines (2033-2045)**

    ---

    **Probability:** 50-60% | **Impact:** High

    Most expert forecasts cluster here, providing time to build institutions but not indefinitely. This window enables learning from near-term AI deployment, developing evaluation and safety capabilities, and establishing governance norms. The key risk is complacency if urgency fades.

    **Planning implication:** Systematic capacity building, learning from near-term deployment, maintaining urgency

-   :material-clock:{ .lg .middle } **Long timelines (2046-2070+)**

    ---

    **Probability:** 10-20% | **Impact:** Still significant

    If AGI requires fundamental breakthroughs, near-term advanced AI still transforms work, economy, and security. Early capability building becomes institutional knowledge. Once other countries deploy advanced systems, Australia must respond regardless.

    **Planning implication:** Preparation compounds over time; don't delay due to timeline uncertainty

</div>

**The key question isn't "when exactly?" but "are we prepared if it's sooner than expected?"** Australia needs more lead time than larger economies due to geographic isolation, small domestic AI industry, and institutional capability gaps.

---

## What to watch: Leading indicators

Rather than relying on predictions, monitor these concrete signals that timelines are shortening or lengthening:

!!! info "Capability indicators signal timeline compression"

    **Human-level performance on complex reasoning** (mathematics, coding, scientific research) | **Autonomous agents completing multi-week projects** with minimal intervention | **AI systems designing improved AI systems** (recursive improvement threshold) | **Economic impact** from AI-driven productivity gains across white-collar work

!!! warning "Deployment patterns reveal competitive pressure"

    **Frontier models integrated into critical infrastructure** (energy, finance, defence) | **Autonomous AI agents at scale** in high-stakes environments | **International competition driving deployment** before safety validation | **Capability proliferation** to smaller actors

!!! question "Safety and governance signals show coordination (or lack thereof)"

    **Frontier labs slowing deployment** due to safety concerns | **Major incidents** attributed to misalignment or loss of control | **International coordination** on capability thresholds and licensing | **Regulatory intervention** halting or constraining development

**SafeAI-Aus will track these indicators** and update guidance as evidence accumulates. Subscribe to our [newsletter](../newsletter.md) for updates on significant developments.

---

## Common questions

??? question "If we don't know when AGI will arrive, how can we prepare?"

    **Preparation doesn't require precise timelines—it requires understanding scenarios:**

    Build capabilities useful across timelines: evaluation frameworks, governance institutions, safety research, resilience planning. These have value whether AGI arrives in 2027 or 2047.

    Design adaptive systems that can accelerate or decelerate based on leading indicators. Don't lock into rigid plans assuming a single timeline.

    Focus on irreversible decisions: Some choices (critical infrastructure dependencies, international commitments, capability development) are hard to reverse. Make these carefully.

    **Analogy:** Earthquake preparedness in Australia doesn't require knowing the exact year of the next major quake—it requires building codes, emergency plans, and monitoring systems.

??? question "Aren't frontier lab predictions self-serving marketing?"

    **Partially—but they also reflect genuine information advantages:**

    Frontier labs see internal capability gains before they're public. They have access to data external researchers don't. Their predictions reflect this privileged information.

    But they also have incentives to appear cutting-edge for talent recruitment and investment. "AGI in 5 years" attracts resources, even if actual timelines are uncertain.

    **Best approach:** Take frontier lab predictions seriously as lower-bound scenarios (they might be right) while maintaining scepticism about marketing-driven optimism. Plan for a range that includes their forecasts.

??? question "What if AGI never arrives? Is this all wasted effort?"

    **Near-term advanced AI still creates most of the governance challenges:**

    Even without "true AGI," systems that automate most knowledge work transform employment, concentrate power, and create security risks. Preparing for AGI means preparing for this transformation.

    Evaluation capabilities, governance frameworks, and resilience planning have value for current AI adoption—they're not contingent on AGI arriving.

    **If AGI never arrives, we still need this work.** If it does arrive, we can't afford to be unprepared.

??? question "Should we slow down AI development to extend timelines?"

    **This is one of the most contentious questions in AI governance:**

    **Arguments for slowing down:**
    - Buys time for safety research and governance to catch up
    - Reduces risk of deploying misaligned systems under competitive pressure
    - Allows society to adapt to transformation rather than being overwhelmed

    **Arguments against:**
    - Unilateral slowdowns cede advantage to competitors (China, other countries)
    - AI provides genuine benefits (medical research, climate science, productivity)
    - Slowing down may not be technically or politically feasible

    **SafeAI-Aus position:** Australia should advocate for **coordinated international capability thresholds** (don't deploy systems above certain risk levels without safety validation) rather than blanket slowdowns. We should also build domestic evaluation capabilities so Australia can independently verify safety claims.

    See our [C·A·G·R Framework](../framework/index.md) for more on containment, alignment, governance and resilience approaches.

??? question "This feels overwhelming. Where should I start?"

    **Start with understanding, not implementation:**

    1. **Read the [AGI Scenarios](../agi-scenarios/index.md)** to understand how risks could unfold in Australia
    2. **Review the [C·A·G·R Framework](../framework/index.md)** for a structured approach to preparation
    3. **Identify which pillar matters most** for your role (containment for regulators, resilience for infrastructure operators, etc.)

    **You don't need to solve everything—just understand your organisation's specific risks and preparation needs.**

    **For immediate actions:** See our [core SafeAI-Aus resources](../safety-standards/index.md) for day-to-day AI governance while building longer-term AGI preparedness.

---

??? note "Sources & Further Reading"

    **Frontier lab statements and thought leaders:**
    - OpenAI (2023) ["Planning for AGI and beyond"](https://openai.com/blog/planning-for-agi-and-beyond) — Official company perspective on AGI development
    - Altman, Sam (2025) ["The Gentle Singularity"](https://blog.samaltman.com/the-gentle-singularity) — OpenAI CEO on gradual but transformative AGI arrival this decade
    - Amodei, Dario (2024) ["Machines of Loving Grace"](https://www.darioamodei.com/essay/machines-of-loving-grace) — Anthropic CEO on powerful AI by 2026-2027 and societal transformation
    - Aschenbrenner, Leopold (2024) ["Situational Awareness: The Decade Ahead"](https://situational-awareness.ai/) — Former OpenAI researcher on AGI by 2027 and national security implications
    - DeepMind research publications on AGI timelines and capabilities

    **Expert surveys and forecasting:**
    - [Good Ancestors Policy AGI Survey (2024)](https://www.goodancestors.org.au/agi_survey_results) — 139 AI safety and governance professionals, median timeline 2035
    - [Epoch AI Literature Review of TAI Timelines](https://epoch.ai/blog/literature-review-of-transformative-artificial-intelligence-timelines) — Comprehensive comparison of models and forecasts, 57% probability by 2050
    - [Epoch AI Benchmark Tracking](https://epoch.ai/data) — Data on AI capability trajectories and milestone predictions
    - [Epoch AI "How well did forecasters predict 2025 AI progress?"](https://epoch.ai/gradient-updates/how-well-did-forecasters-predict-2025-ai-progress) — Analysis of forecasting accuracy
    - [AI Impacts HLMI Survey (2022)](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/) — Broader AI researcher community, median 2060
    - [Metaculus AGI forecasts](https://www.metaculus.com/questions/3479/when-will-the-first-general-ai-system-be-devised-tested-and-publicly-known-of/) — Aggregated prediction markets, 50% by 2040-2045

    **Australian and Australia-connected researchers:**
    - Legg, Shane (2009-2024) — DeepMind co-founder, 50% AGI by 2028 prediction maintained since 2009 | [Dwarkesh interview](https://www.dwarkesh.com/p/shane-legg) | [The Decoder analysis](https://the-decoder.com/deepmind-co-founder-shane-legg-sees-50-percent-chance-of-minimal-agi-by-2028/)
    - Hutter, Marcus — ANU Professor, DeepMind senior researcher, AIXI and universal AI theory | [Lex Fridman interview](https://lexfridman.com/marcus-hutter/) | [Google Scholar](https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en)
    - Walsh, Toby — UNSW Scientia Professor, CSIRO Data61, AGI timeline surveys predicting ~2062 | [Griffith News profile](https://news.griffith.edu.au/2023/03/29/toby-walsh/) | [AI Expert Group](https://www.minister.industry.gov.au/ministers/husic/media-releases/new-artificial-intelligence-expert-group)
    - Toner, Helen (2024) ["Unresolved debates about the future of AI"](https://helentoner.substack.com/p/unresolved-debates-about-the-future) — Melbourne-born, former OpenAI board member on three fundamental questions
    - Toner, Helen (2025) ["Long timelines to advanced AI have substantial benefits"](https://helentoner.substack.com/p/long-timelines-to-advanced-ai-have) — Analysis of slower development benefits

    **Academic analysis and commentary:**
    - Cotra, Ajeya (2020) ["Forecasting TAI with biological anchors"](https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines) — Open Philanthropy research
    - Grace et al. (2018) ["When will AI exceed human performance?"](https://arxiv.org/abs/1705.08807) — Survey methodology
    - Ord, Toby (2020) *The Precipice: Existential Risk and the Future of Humanity* — Timeline uncertainty analysis
    - Urban, Tim (2015) ["The AI Revolution"](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) — Wait But Why foundational explainer
    - Coleman, Ben (2024) ["Common ground between AI 2027 and AI 2070"](https://asteriskmag.substack.com/p/common-ground-between-ai-2027-and) — Convergence across perspectives

    **Chinese AI development and international competition:**
    - China AI Briefing (2023) ["China releases Generative AI regulations"](https://www.china-briefing.com/news/china-releases-generative-ai-regulations/) — National strategic context
    - Georgetown CSET China AI research program — Analysis of Chinese frontier AI development
    - RAND Corporation (2025) ["Seeking Stability in the Competition for AI Advantage"](https://www.rand.org/pubs/commentary/2025/03/seeking-stability-in-the-competition-for-ai-advantage.html) — Analysis of US-China AI competition dynamics and stability mechanisms

    **Australian context:**
    - [CSIRO AI Roadmap](https://www.csiro.au/en/research/technology-space/ai) — National AI capability planning
    - [National AI Centre](https://www.csiro.au/en/about/challenges-missions/naic) — Responsible AI research coordination
    - Australian Strategic Policy Institute [Critical Technology Tracker](https://www.aspi.org.au/report/critical-technology-tracker) — International AI capability monitoring

---

[:octicons-arrow-left-24: Back to Preparing for AGI](preparing-for-agi.md)
